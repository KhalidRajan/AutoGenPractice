{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm_config = {\"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": api_key}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tool to Retrieve Papers from Arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from typing import List, Dict, Any\n",
    "from datetime import  datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = arxiv.Search(\n",
    "    query = \"Fine tuning transformer models\",\n",
    "    max_results = 10,\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entry_id': 'http://arxiv.org/abs/2501.01957v1', 'updated': datetime.datetime(2025, 1, 3, 18, 59, 52, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 59, 52, tzinfo=datetime.timezone.utc), 'title': 'VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction', 'authors': [arxiv.Result.Author('Chaoyou Fu'), arxiv.Result.Author('Haojia Lin'), arxiv.Result.Author('Xiong Wang'), arxiv.Result.Author('Yi-Fan Zhang'), arxiv.Result.Author('Yunhang Shen'), arxiv.Result.Author('Xiaoyu Liu'), arxiv.Result.Author('Yangze Li'), arxiv.Result.Author('Zuwei Long'), arxiv.Result.Author('Heting Gao'), arxiv.Result.Author('Ke Li'), arxiv.Result.Author('Xiawu Zheng'), arxiv.Result.Author('Rongrong Ji'), arxiv.Result.Author('Xing Sun'), arxiv.Result.Author('Caifeng Shan'), arxiv.Result.Author('Ran He')], 'summary': 'Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.', 'comment': 'https://github.com/VITA-MLLM/VITA', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV', 'cs.SD', 'eess.AS'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01957v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01957v1', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01957v1', '_raw': {'id': 'http://arxiv.org/abs/2501.01957v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01957v1', 'updated': '2025-01-03T18:59:52Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=59, tm_sec=52, tm_wday=4, tm_yday=3, tm_isdst=0), 'published': '2025-01-03T18:59:52Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=59, tm_sec=52, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction'}, 'summary': 'Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.'}, 'authors': [{'name': 'Chaoyou Fu'}, {'name': 'Haojia Lin'}, {'name': 'Xiong Wang'}, {'name': 'Yi-Fan Zhang'}, {'name': 'Yunhang Shen'}, {'name': 'Xiaoyu Liu'}, {'name': 'Yangze Li'}, {'name': 'Zuwei Long'}, {'name': 'Heting Gao'}, {'name': 'Ke Li'}, {'name': 'Xiawu Zheng'}, {'name': 'Rongrong Ji'}, {'name': 'Xing Sun'}, {'name': 'Caifeng Shan'}, {'name': 'Ran He'}], 'author_detail': {'name': 'Ran He'}, 'author': 'Ran He', 'arxiv_comment': 'https://github.com/VITA-MLLM/VITA', 'links': [{'href': 'http://arxiv.org/abs/2501.01957v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01957v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n",
      "{'entry_id': 'http://arxiv.org/abs/2501.01956v1', 'updated': datetime.datetime(2025, 1, 3, 18, 59, 23, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 59, 23, tzinfo=datetime.timezone.utc), 'title': 'Metadata Conditioning Accelerates Language Model Pre-training', 'authors': [arxiv.Result.Author('Tianyu Gao'), arxiv.Result.Author('Alexander Wettig'), arxiv.Result.Author('Luxi He'), arxiv.Result.Author('Yihe Dong'), arxiv.Result.Author('Sadhika Malladi'), arxiv.Result.Author('Danqi Chen')], 'summary': 'The vast diversity of styles, domains, and quality levels present in language\\nmodel pre-training corpora is essential in developing general model\\ncapabilities, but efficiently learning and deploying the correct behaviors\\nexemplified in each of these heterogeneous data sources is challenging. To\\naddress this, we propose a new method, termed Metadata Conditioning then\\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\\ntext during training and later uses a cooldown phase with only the standard\\ntext, thereby enabling the model to function normally even without metadata.\\nMeCo significantly accelerates pre-training across different model scales (600M\\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\\ninstance, a 1.6B language model trained with MeCo matches the downstream task\\nperformance of standard pre-training while using 33% less data. Additionally,\\nMeCo enables us to steer language models by conditioning the inference prompt\\non either real or fabricated metadata that encodes the desired properties of\\nthe output: for example, prepending wikipedia.org to reduce harmful generations\\nor factquizmaster.com (fabricated) to improve common knowledge task\\nperformance. We also demonstrate that MeCo is compatible with different types\\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\\ncomputational overhead, and demonstrates promise in producing more capable and\\nsteerable language models.', 'comment': 'Code available at https://github.com/princeton-pli/MeCo', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01956v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01956v1', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01956v1', '_raw': {'id': 'http://arxiv.org/abs/2501.01956v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01956v1', 'updated': '2025-01-03T18:59:23Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=59, tm_sec=23, tm_wday=4, tm_yday=3, tm_isdst=0), 'published': '2025-01-03T18:59:23Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=59, tm_sec=23, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'Metadata Conditioning Accelerates Language Model Pre-training', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Metadata Conditioning Accelerates Language Model Pre-training'}, 'summary': 'The vast diversity of styles, domains, and quality levels present in language\\nmodel pre-training corpora is essential in developing general model\\ncapabilities, but efficiently learning and deploying the correct behaviors\\nexemplified in each of these heterogeneous data sources is challenging. To\\naddress this, we propose a new method, termed Metadata Conditioning then\\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\\ntext during training and later uses a cooldown phase with only the standard\\ntext, thereby enabling the model to function normally even without metadata.\\nMeCo significantly accelerates pre-training across different model scales (600M\\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\\ninstance, a 1.6B language model trained with MeCo matches the downstream task\\nperformance of standard pre-training while using 33% less data. Additionally,\\nMeCo enables us to steer language models by conditioning the inference prompt\\non either real or fabricated metadata that encodes the desired properties of\\nthe output: for example, prepending wikipedia.org to reduce harmful generations\\nor factquizmaster.com (fabricated) to improve common knowledge task\\nperformance. We also demonstrate that MeCo is compatible with different types\\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\\ncomputational overhead, and demonstrates promise in producing more capable and\\nsteerable language models.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The vast diversity of styles, domains, and quality levels present in language\\nmodel pre-training corpora is essential in developing general model\\ncapabilities, but efficiently learning and deploying the correct behaviors\\nexemplified in each of these heterogeneous data sources is challenging. To\\naddress this, we propose a new method, termed Metadata Conditioning then\\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\\ntext during training and later uses a cooldown phase with only the standard\\ntext, thereby enabling the model to function normally even without metadata.\\nMeCo significantly accelerates pre-training across different model scales (600M\\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\\ninstance, a 1.6B language model trained with MeCo matches the downstream task\\nperformance of standard pre-training while using 33% less data. Additionally,\\nMeCo enables us to steer language models by conditioning the inference prompt\\non either real or fabricated metadata that encodes the desired properties of\\nthe output: for example, prepending wikipedia.org to reduce harmful generations\\nor factquizmaster.com (fabricated) to improve common knowledge task\\nperformance. We also demonstrate that MeCo is compatible with different types\\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\\ncomputational overhead, and demonstrates promise in producing more capable and\\nsteerable language models.'}, 'authors': [{'name': 'Tianyu Gao'}, {'name': 'Alexander Wettig'}, {'name': 'Luxi He'}, {'name': 'Yihe Dong'}, {'name': 'Sadhika Malladi'}, {'name': 'Danqi Chen'}], 'author_detail': {'name': 'Danqi Chen'}, 'author': 'Danqi Chen', 'arxiv_comment': 'Code available at https://github.com/princeton-pli/MeCo', 'links': [{'href': 'http://arxiv.org/abs/2501.01956v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01956v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n",
      "{'entry_id': 'http://arxiv.org/abs/2501.01953v2', 'updated': datetime.datetime(2025, 1, 6, 18, 55, 10, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 58, 19, tzinfo=datetime.timezone.utc), 'title': 'Quantum Error Correction Without Encoding via the Circulant Structure of Pauli Noise and the Fast Fourier Transform', 'authors': [arxiv.Result.Author('Alvin Gonzales')], 'summary': 'This work introduces a method for correcting the output distribution of a\\nquantum computer that does not require encoding of the logical qubits into more\\nphysical qubits. Thus, it avoids the encoding overhead of standard quantum\\nerror correction codes. If the noise affecting the circuit is a Pauli channel\\n(we can bias the noise with twirling), the ideal output distribution and noisy\\ndistribution in the standard basis are related by a stochastic matrix. We prove\\nthat this matrix has a circulant block structure. Thus, the ideal distribution\\ncan be retrieved from the noisy output distribution by applying the Fast\\nFourier Transform. Moreover, due to its circulant structure, characterization\\nof this matrix can be achieved by sampling a single circuit. The results are\\ncorroborated with quantum hardware executions consisting of 20-qubit and\\n30-qubit GHZ state preparation, 5-qubit Grover, 6-qubit and 10-qubit quantum\\nphase estimation, and 10-qubit and 20-qubit Dicke state preparation circuits.\\nThe correction process dramatically improves the accuracies of the output\\ndistributions. For 30-qubit GHZ state preparation, a corrected distribution\\nfidelity of 97.7% is achieved from an initial raw fidelity of 23.2%.', 'comment': 'Comments are welcome!', 'journal_ref': None, 'doi': None, 'primary_category': 'quant-ph', 'categories': ['quant-ph'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01953v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01953v2', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01953v2', '_raw': {'id': 'http://arxiv.org/abs/2501.01953v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01953v2', 'updated': '2025-01-06T18:55:10Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=6, tm_hour=18, tm_min=55, tm_sec=10, tm_wday=0, tm_yday=6, tm_isdst=0), 'published': '2025-01-03T18:58:19Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=58, tm_sec=19, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'Quantum Error Correction Without Encoding via the Circulant Structure of\\n  Pauli Noise and the Fast Fourier Transform', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Quantum Error Correction Without Encoding via the Circulant Structure of\\n  Pauli Noise and the Fast Fourier Transform'}, 'summary': 'This work introduces a method for correcting the output distribution of a\\nquantum computer that does not require encoding of the logical qubits into more\\nphysical qubits. Thus, it avoids the encoding overhead of standard quantum\\nerror correction codes. If the noise affecting the circuit is a Pauli channel\\n(we can bias the noise with twirling), the ideal output distribution and noisy\\ndistribution in the standard basis are related by a stochastic matrix. We prove\\nthat this matrix has a circulant block structure. Thus, the ideal distribution\\ncan be retrieved from the noisy output distribution by applying the Fast\\nFourier Transform. Moreover, due to its circulant structure, characterization\\nof this matrix can be achieved by sampling a single circuit. The results are\\ncorroborated with quantum hardware executions consisting of 20-qubit and\\n30-qubit GHZ state preparation, 5-qubit Grover, 6-qubit and 10-qubit quantum\\nphase estimation, and 10-qubit and 20-qubit Dicke state preparation circuits.\\nThe correction process dramatically improves the accuracies of the output\\ndistributions. For 30-qubit GHZ state preparation, a corrected distribution\\nfidelity of 97.7% is achieved from an initial raw fidelity of 23.2%.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'This work introduces a method for correcting the output distribution of a\\nquantum computer that does not require encoding of the logical qubits into more\\nphysical qubits. Thus, it avoids the encoding overhead of standard quantum\\nerror correction codes. If the noise affecting the circuit is a Pauli channel\\n(we can bias the noise with twirling), the ideal output distribution and noisy\\ndistribution in the standard basis are related by a stochastic matrix. We prove\\nthat this matrix has a circulant block structure. Thus, the ideal distribution\\ncan be retrieved from the noisy output distribution by applying the Fast\\nFourier Transform. Moreover, due to its circulant structure, characterization\\nof this matrix can be achieved by sampling a single circuit. The results are\\ncorroborated with quantum hardware executions consisting of 20-qubit and\\n30-qubit GHZ state preparation, 5-qubit Grover, 6-qubit and 10-qubit quantum\\nphase estimation, and 10-qubit and 20-qubit Dicke state preparation circuits.\\nThe correction process dramatically improves the accuracies of the output\\ndistributions. For 30-qubit GHZ state preparation, a corrected distribution\\nfidelity of 97.7% is achieved from an initial raw fidelity of 23.2%.'}, 'authors': [{'name': 'Alvin Gonzales'}], 'author_detail': {'name': 'Alvin Gonzales'}, 'author': 'Alvin Gonzales', 'arxiv_comment': 'Comments are welcome!', 'links': [{'href': 'http://arxiv.org/abs/2501.01953v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01953v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'quant-ph', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'quant-ph', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n",
      "{'entry_id': 'http://arxiv.org/abs/2501.01951v2', 'updated': datetime.datetime(2025, 1, 6, 6, 46, 7, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 54, 46, tzinfo=datetime.timezone.utc), 'title': 'MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of Accelerators', 'authors': [arxiv.Result.Author('Cheng Wan'), arxiv.Result.Author('Runkai Tao'), arxiv.Result.Author('Zheng Du'), arxiv.Result.Author('Yang Katie Zhao'), arxiv.Result.Author('Yingyan Celine Lin')], 'summary': 'Graph convolutional networks (GCNs) have demonstrated superiority in\\ngraph-based learning tasks. However, training GCNs on full graphs is\\nparticularly challenging, due to the following two challenges: (1) the\\nassociated feature tensors can easily explode the memory and block the\\ncommunication bandwidth of modern accelerators, and (2) the computation\\nworkflow in training GCNs alternates between sparse and dense matrix\\noperations, complicating the efficient utilization of computational resources.\\nExisting solutions for scalable distributed full-graph GCN training mostly\\nadopt partition parallelism, which is unsatisfactory as they only partially\\naddress the first challenge while incurring scaled-out communication volume. To\\nthis end, we propose MixGCN aiming to simultaneously address both the\\naforementioned challenges towards GCN training. To tackle the first challenge,\\nMixGCN integrates mixture of parallelism. Both theoretical and empirical\\nanalysis verify its constant communication volumes and enhanced balanced\\nworkload; For handling the second challenge, we consider mixture of\\naccelerators (i.e., sparse and dense accelerators) with a dedicated accelerator\\nfor GCN training and a fine-grain pipeline. Extensive experiments show that\\nMixGCN achieves boosted training efficiency and scalability.', 'comment': '15 pages, 12 figures, 5 tables', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.AI'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01951v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01951v2', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01951v2', '_raw': {'id': 'http://arxiv.org/abs/2501.01951v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01951v2', 'updated': '2025-01-06T06:46:07Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=6, tm_hour=6, tm_min=46, tm_sec=7, tm_wday=0, tm_yday=6, tm_isdst=0), 'published': '2025-01-03T18:54:46Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=54, tm_sec=46, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of\\n  Accelerators', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of\\n  Accelerators'}, 'summary': 'Graph convolutional networks (GCNs) have demonstrated superiority in\\ngraph-based learning tasks. However, training GCNs on full graphs is\\nparticularly challenging, due to the following two challenges: (1) the\\nassociated feature tensors can easily explode the memory and block the\\ncommunication bandwidth of modern accelerators, and (2) the computation\\nworkflow in training GCNs alternates between sparse and dense matrix\\noperations, complicating the efficient utilization of computational resources.\\nExisting solutions for scalable distributed full-graph GCN training mostly\\nadopt partition parallelism, which is unsatisfactory as they only partially\\naddress the first challenge while incurring scaled-out communication volume. To\\nthis end, we propose MixGCN aiming to simultaneously address both the\\naforementioned challenges towards GCN training. To tackle the first challenge,\\nMixGCN integrates mixture of parallelism. Both theoretical and empirical\\nanalysis verify its constant communication volumes and enhanced balanced\\nworkload; For handling the second challenge, we consider mixture of\\naccelerators (i.e., sparse and dense accelerators) with a dedicated accelerator\\nfor GCN training and a fine-grain pipeline. Extensive experiments show that\\nMixGCN achieves boosted training efficiency and scalability.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Graph convolutional networks (GCNs) have demonstrated superiority in\\ngraph-based learning tasks. However, training GCNs on full graphs is\\nparticularly challenging, due to the following two challenges: (1) the\\nassociated feature tensors can easily explode the memory and block the\\ncommunication bandwidth of modern accelerators, and (2) the computation\\nworkflow in training GCNs alternates between sparse and dense matrix\\noperations, complicating the efficient utilization of computational resources.\\nExisting solutions for scalable distributed full-graph GCN training mostly\\nadopt partition parallelism, which is unsatisfactory as they only partially\\naddress the first challenge while incurring scaled-out communication volume. To\\nthis end, we propose MixGCN aiming to simultaneously address both the\\naforementioned challenges towards GCN training. To tackle the first challenge,\\nMixGCN integrates mixture of parallelism. Both theoretical and empirical\\nanalysis verify its constant communication volumes and enhanced balanced\\nworkload; For handling the second challenge, we consider mixture of\\naccelerators (i.e., sparse and dense accelerators) with a dedicated accelerator\\nfor GCN training and a fine-grain pipeline. Extensive experiments show that\\nMixGCN achieves boosted training efficiency and scalability.'}, 'authors': [{'name': 'Cheng Wan'}, {'name': 'Runkai Tao'}, {'name': 'Zheng Du'}, {'name': 'Yang Katie Zhao'}, {'name': 'Yingyan Celine Lin'}], 'author_detail': {'name': 'Yingyan Celine Lin'}, 'author': 'Yingyan Celine Lin', 'arxiv_comment': '15 pages, 12 figures, 5 tables', 'links': [{'href': 'http://arxiv.org/abs/2501.01951v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01951v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n",
      "{'entry_id': 'http://arxiv.org/abs/2501.01950v1', 'updated': datetime.datetime(2025, 1, 3, 18, 54, 26, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 54, 26, tzinfo=datetime.timezone.utc), 'title': 'MADGEN -- Mass-Spec attends to De Novo Molecular generation', 'authors': [arxiv.Result.Author('Yinkai Wang'), arxiv.Result.Author('Xiaohui Chen'), arxiv.Result.Author('Liping Liu'), arxiv.Result.Author('Soha Hassoun')], 'summary': 'The annotation (assigning structural chemical identities) of MS/MS spectra\\nremains a significant challenge due to the enormous molecular diversity in\\nbiological samples and the limited scope of reference databases. Currently, the\\nvast majority of spectral measurements remain in the \"dark chemical space\"\\nwithout structural annotations. To improve annotation, we propose MADGEN\\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\\nfor de novo molecular structure generation guided by mass spectrometry data.\\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\\nmolecular generation starting with the scaffold. In the first stage, given an\\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\\nspectrum to guide an attention-based generative model to generate the final\\nmolecule. Our approach constrains the molecular generation search space,\\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN\\'s\\nperformance with a predictive scaffold retriever and with an oracle retriever.\\nWe demonstrate the effectiveness of using attention to integrate spectral\\ninformation throughout the generation process to achieve strong results with\\nthe oracle retriever.', 'comment': 'preprint', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.AI'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01950v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01950v1', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01950v1', '_raw': {'id': 'http://arxiv.org/abs/2501.01950v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01950v1', 'updated': '2025-01-03T18:54:26Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=54, tm_sec=26, tm_wday=4, tm_yday=3, tm_isdst=0), 'published': '2025-01-03T18:54:26Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=54, tm_sec=26, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'MADGEN -- Mass-Spec attends to De Novo Molecular generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'MADGEN -- Mass-Spec attends to De Novo Molecular generation'}, 'summary': 'The annotation (assigning structural chemical identities) of MS/MS spectra\\nremains a significant challenge due to the enormous molecular diversity in\\nbiological samples and the limited scope of reference databases. Currently, the\\nvast majority of spectral measurements remain in the \"dark chemical space\"\\nwithout structural annotations. To improve annotation, we propose MADGEN\\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\\nfor de novo molecular structure generation guided by mass spectrometry data.\\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\\nmolecular generation starting with the scaffold. In the first stage, given an\\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\\nspectrum to guide an attention-based generative model to generate the final\\nmolecule. Our approach constrains the molecular generation search space,\\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN\\'s\\nperformance with a predictive scaffold retriever and with an oracle retriever.\\nWe demonstrate the effectiveness of using attention to integrate spectral\\ninformation throughout the generation process to achieve strong results with\\nthe oracle retriever.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The annotation (assigning structural chemical identities) of MS/MS spectra\\nremains a significant challenge due to the enormous molecular diversity in\\nbiological samples and the limited scope of reference databases. Currently, the\\nvast majority of spectral measurements remain in the \"dark chemical space\"\\nwithout structural annotations. To improve annotation, we propose MADGEN\\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\\nfor de novo molecular structure generation guided by mass spectrometry data.\\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\\nmolecular generation starting with the scaffold. In the first stage, given an\\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\\nspectrum to guide an attention-based generative model to generate the final\\nmolecule. Our approach constrains the molecular generation search space,\\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN\\'s\\nperformance with a predictive scaffold retriever and with an oracle retriever.\\nWe demonstrate the effectiveness of using attention to integrate spectral\\ninformation throughout the generation process to achieve strong results with\\nthe oracle retriever.'}, 'authors': [{'name': 'Yinkai Wang'}, {'name': 'Xiaohui Chen'}, {'name': 'Liping Liu'}, {'name': 'Soha Hassoun'}], 'author_detail': {'name': 'Soha Hassoun'}, 'author': 'Soha Hassoun', 'arxiv_comment': 'preprint', 'links': [{'href': 'http://arxiv.org/abs/2501.01950v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01950v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n",
      "{'entry_id': 'http://arxiv.org/abs/2501.01949v1', 'updated': datetime.datetime(2025, 1, 3, 18, 52, 36, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 52, 36, tzinfo=datetime.timezone.utc), 'title': 'VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment', 'authors': [arxiv.Result.Author('Wenyan Cong'), arxiv.Result.Author('Kevin Wang'), arxiv.Result.Author('Jiahui Lei'), arxiv.Result.Author('Colton Stearns'), arxiv.Result.Author('Yuanhao Cai'), arxiv.Result.Author('Dilin Wang'), arxiv.Result.Author('Rakesh Ranjan'), arxiv.Result.Author('Matt Feiszli'), arxiv.Result.Author('Leonidas Guibas'), arxiv.Result.Author('Zhangyang Wang'), arxiv.Result.Author('Weiyao Wang'), arxiv.Result.Author('Zhiwen Fan')], 'summary': 'Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.', 'comment': 'project page: https://videolifter.github.io', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CV', 'categories': ['cs.CV'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01949v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01949v1', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01949v1', '_raw': {'id': 'http://arxiv.org/abs/2501.01949v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01949v1', 'updated': '2025-01-03T18:52:36Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=52, tm_sec=36, tm_wday=4, tm_yday=3, tm_isdst=0), 'published': '2025-01-03T18:52:36Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=52, tm_sec=36, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\\n  Alignment', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\\n  Alignment'}, 'summary': 'Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.'}, 'authors': [{'name': 'Wenyan Cong'}, {'name': 'Kevin Wang'}, {'name': 'Jiahui Lei'}, {'name': 'Colton Stearns'}, {'name': 'Yuanhao Cai'}, {'name': 'Dilin Wang'}, {'name': 'Rakesh Ranjan'}, {'name': 'Matt Feiszli'}, {'name': 'Leonidas Guibas'}, {'name': 'Zhangyang Wang'}, {'name': 'Weiyao Wang'}, {'name': 'Zhiwen Fan'}], 'author_detail': {'name': 'Zhiwen Fan'}, 'author': 'Zhiwen Fan', 'arxiv_comment': 'project page: https://videolifter.github.io', 'links': [{'href': 'http://arxiv.org/abs/2501.01949v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01949v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n",
      "{'entry_id': 'http://arxiv.org/abs/2501.01948v1', 'updated': datetime.datetime(2025, 1, 3, 18, 52, 6, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 52, 6, tzinfo=datetime.timezone.utc), 'title': 'A New Approach to the Analysis of Cosmological Parameters in Multifield Cosmology', 'authors': [arxiv.Result.Author('Katerina Bolshakova'), arxiv.Result.Author('Sergey Chervon')], 'summary': 'Currently, a method has been developed for the cosmological inflation model\\nwith a single scalar field to calculate cosmological parameters such as the\\npower spectrum of scalar and tensor perturbations, their spectral indices, and\\nthe tensor-to-scalar ratio. However, for a multifield configuration, a\\ndefinitive method for calculating cosmological parameters has not yet been\\nestablished.\\n  We propose a new effective algorithm for determining cosmological parameters\\nwithin the tensor-multi-scalar theory of gravity, which describes the early\\ninflationary epoch of the Universe. Our approach is based on utilizing a\\nspecific analytical solution within a multifield cosmological model to\\nestablish functional relationships between fields. This method enables the\\ncomputation of cosmological parameters and their comparison with observational\\ndata.', 'comment': '12 pages', 'journal_ref': 'Nonlinear World, 2024, Vol. 22, No. 3, pp. 91-103', 'doi': None, 'primary_category': 'gr-qc', 'categories': ['gr-qc'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01948v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01948v1', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01948v1', '_raw': {'id': 'http://arxiv.org/abs/2501.01948v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01948v1', 'updated': '2025-01-03T18:52:06Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=52, tm_sec=6, tm_wday=4, tm_yday=3, tm_isdst=0), 'published': '2025-01-03T18:52:06Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=52, tm_sec=6, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'A New Approach to the Analysis of Cosmological Parameters in Multifield\\n  Cosmology', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A New Approach to the Analysis of Cosmological Parameters in Multifield\\n  Cosmology'}, 'summary': 'Currently, a method has been developed for the cosmological inflation model\\nwith a single scalar field to calculate cosmological parameters such as the\\npower spectrum of scalar and tensor perturbations, their spectral indices, and\\nthe tensor-to-scalar ratio. However, for a multifield configuration, a\\ndefinitive method for calculating cosmological parameters has not yet been\\nestablished.\\n  We propose a new effective algorithm for determining cosmological parameters\\nwithin the tensor-multi-scalar theory of gravity, which describes the early\\ninflationary epoch of the Universe. Our approach is based on utilizing a\\nspecific analytical solution within a multifield cosmological model to\\nestablish functional relationships between fields. This method enables the\\ncomputation of cosmological parameters and their comparison with observational\\ndata.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Currently, a method has been developed for the cosmological inflation model\\nwith a single scalar field to calculate cosmological parameters such as the\\npower spectrum of scalar and tensor perturbations, their spectral indices, and\\nthe tensor-to-scalar ratio. However, for a multifield configuration, a\\ndefinitive method for calculating cosmological parameters has not yet been\\nestablished.\\n  We propose a new effective algorithm for determining cosmological parameters\\nwithin the tensor-multi-scalar theory of gravity, which describes the early\\ninflationary epoch of the Universe. Our approach is based on utilizing a\\nspecific analytical solution within a multifield cosmological model to\\nestablish functional relationships between fields. This method enables the\\ncomputation of cosmological parameters and their comparison with observational\\ndata.'}, 'authors': [{'name': 'Katerina Bolshakova'}, {'name': 'Sergey Chervon'}], 'author_detail': {'name': 'Sergey Chervon'}, 'author': 'Sergey Chervon', 'arxiv_comment': '12 pages', 'arxiv_journal_ref': 'Nonlinear World, 2024, Vol. 22, No. 3, pp. 91-103', 'links': [{'href': 'http://arxiv.org/abs/2501.01948v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01948v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'gr-qc', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'gr-qc', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n",
      "{'entry_id': 'http://arxiv.org/abs/2501.01945v1', 'updated': datetime.datetime(2025, 1, 3, 18, 51, 18, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 51, 18, tzinfo=datetime.timezone.utc), 'title': 'Cold-Start Recommendation towards the Era of Large Language Models (LLMs): A Comprehensive Survey and Roadmap', 'authors': [arxiv.Result.Author('Weizhi Zhang'), arxiv.Result.Author('Yuanchen Bei'), arxiv.Result.Author('Liangwei Yang'), arxiv.Result.Author('Henry Peng Zou'), arxiv.Result.Author('Peilin Zhou'), arxiv.Result.Author('Aiwei Liu'), arxiv.Result.Author('Yinghui Li'), arxiv.Result.Author('Hao Chen'), arxiv.Result.Author('Jianling Wang'), arxiv.Result.Author('Yu Wang'), arxiv.Result.Author('Feiran Huang'), arxiv.Result.Author('Sheng Zhou'), arxiv.Result.Author('Jiajun Bu'), arxiv.Result.Author('Allen Lin'), arxiv.Result.Author('James Caverlee'), arxiv.Result.Author('Fakhri Karray'), arxiv.Result.Author('Irwin King'), arxiv.Result.Author('Philip S. Yu')], 'summary': 'Cold-start problem is one of the long-standing challenges in recommender\\nsystems, focusing on accurately modeling new or interaction-limited users or\\nitems to provide better recommendations. Due to the diversification of internet\\nplatforms and the exponential growth of users and items, the importance of\\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\\ntime, large language models (LLMs) have achieved tremendous success and possess\\nstrong capabilities in modeling user and item information, providing new\\npotential for cold-start recommendations. However, the research community on\\nCSR still lacks a comprehensive review and reflection in this field. Based on\\nthis, in this paper, we stand in the context of the era of large language\\nmodels and provide a comprehensive review and discussion on the roadmap,\\nrelated literature, and future directions of CSR. Specifically, we have\\nconducted an exploration of the development path of how existing CSR utilizes\\ninformation, from content features, graph relations, and domain information, to\\nthe world knowledge possessed by large language models, aiming to provide new\\ninsights for both the research and industrial communities on CSR. Related\\nresources of cold-start recommendations are collected and continuously updated\\nfor the community in\\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.IR', 'categories': ['cs.IR', 'cs.AI'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01945v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01945v1', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01945v1', '_raw': {'id': 'http://arxiv.org/abs/2501.01945v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01945v1', 'updated': '2025-01-03T18:51:18Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=51, tm_sec=18, tm_wday=4, tm_yday=3, tm_isdst=0), 'published': '2025-01-03T18:51:18Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=51, tm_sec=18, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'Cold-Start Recommendation towards the Era of Large Language Models\\n  (LLMs): A Comprehensive Survey and Roadmap', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Cold-Start Recommendation towards the Era of Large Language Models\\n  (LLMs): A Comprehensive Survey and Roadmap'}, 'summary': 'Cold-start problem is one of the long-standing challenges in recommender\\nsystems, focusing on accurately modeling new or interaction-limited users or\\nitems to provide better recommendations. Due to the diversification of internet\\nplatforms and the exponential growth of users and items, the importance of\\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\\ntime, large language models (LLMs) have achieved tremendous success and possess\\nstrong capabilities in modeling user and item information, providing new\\npotential for cold-start recommendations. However, the research community on\\nCSR still lacks a comprehensive review and reflection in this field. Based on\\nthis, in this paper, we stand in the context of the era of large language\\nmodels and provide a comprehensive review and discussion on the roadmap,\\nrelated literature, and future directions of CSR. Specifically, we have\\nconducted an exploration of the development path of how existing CSR utilizes\\ninformation, from content features, graph relations, and domain information, to\\nthe world knowledge possessed by large language models, aiming to provide new\\ninsights for both the research and industrial communities on CSR. Related\\nresources of cold-start recommendations are collected and continuously updated\\nfor the community in\\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Cold-start problem is one of the long-standing challenges in recommender\\nsystems, focusing on accurately modeling new or interaction-limited users or\\nitems to provide better recommendations. Due to the diversification of internet\\nplatforms and the exponential growth of users and items, the importance of\\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\\ntime, large language models (LLMs) have achieved tremendous success and possess\\nstrong capabilities in modeling user and item information, providing new\\npotential for cold-start recommendations. However, the research community on\\nCSR still lacks a comprehensive review and reflection in this field. Based on\\nthis, in this paper, we stand in the context of the era of large language\\nmodels and provide a comprehensive review and discussion on the roadmap,\\nrelated literature, and future directions of CSR. Specifically, we have\\nconducted an exploration of the development path of how existing CSR utilizes\\ninformation, from content features, graph relations, and domain information, to\\nthe world knowledge possessed by large language models, aiming to provide new\\ninsights for both the research and industrial communities on CSR. Related\\nresources of cold-start recommendations are collected and continuously updated\\nfor the community in\\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.'}, 'authors': [{'name': 'Weizhi Zhang'}, {'name': 'Yuanchen Bei'}, {'name': 'Liangwei Yang'}, {'name': 'Henry Peng Zou'}, {'name': 'Peilin Zhou'}, {'name': 'Aiwei Liu'}, {'name': 'Yinghui Li'}, {'name': 'Hao Chen'}, {'name': 'Jianling Wang'}, {'name': 'Yu Wang'}, {'name': 'Feiran Huang'}, {'name': 'Sheng Zhou'}, {'name': 'Jiajun Bu'}, {'name': 'Allen Lin'}, {'name': 'James Caverlee'}, {'name': 'Fakhri Karray'}, {'name': 'Irwin King'}, {'name': 'Philip S. Yu'}], 'author_detail': {'name': 'Philip S. Yu'}, 'author': 'Philip S. Yu', 'links': [{'href': 'http://arxiv.org/abs/2501.01945v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01945v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n",
      "{'entry_id': 'http://arxiv.org/abs/2501.01944v1', 'updated': datetime.datetime(2025, 1, 3, 18, 50, 52, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 50, 52, tzinfo=datetime.timezone.utc), 'title': 'Quasi-topological fractons: a 3D dipolar gauge theory', 'authors': [arxiv.Result.Author('Erica Bertolini'), arxiv.Result.Author('Alberto Blasi'), arxiv.Result.Author('Nicola Maggiore')], 'summary': 'We consider the theory of a generic rank-2 tensor field in three spacetime\\ndimensions, which involves a symmetric tensor field transforming under\\ninfinitesimal diffeomorphisms, and a vector field, whose gauge transformation\\ndepends on a local vector parameter. The gauge fixing shows a non-trivial\\nstructure, and some non-intuitive possibilities are listed. Despite the fact\\nthat the theory is not topological, the energy-momentum tensor vanishes\\non-shell, which justifies the quasi-topological appellation we give to this\\ntheory. We show that the theory has three degrees of freedom. Moreover we find\\nan interesting physical interpretation, which consists in a generalized planar\\nelectromagnetism and in the emergence of two vector charges with restricted\\nmobility. These are typical fractonic behaviours which can be related to the so\\ncalled traceless scalar and vector charge theories.', 'comment': '30 pages, version to appear on The European Physical Journal C (EPJ\\n  C)', 'journal_ref': None, 'doi': None, 'primary_category': 'hep-th', 'categories': ['hep-th', 'cond-mat.mes-hall', 'cond-mat.str-el', 'gr-qc'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01944v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01944v1', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01944v1', '_raw': {'id': 'http://arxiv.org/abs/2501.01944v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01944v1', 'updated': '2025-01-03T18:50:52Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=50, tm_sec=52, tm_wday=4, tm_yday=3, tm_isdst=0), 'published': '2025-01-03T18:50:52Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=50, tm_sec=52, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'Quasi-topological fractons: a 3D dipolar gauge theory', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Quasi-topological fractons: a 3D dipolar gauge theory'}, 'summary': 'We consider the theory of a generic rank-2 tensor field in three spacetime\\ndimensions, which involves a symmetric tensor field transforming under\\ninfinitesimal diffeomorphisms, and a vector field, whose gauge transformation\\ndepends on a local vector parameter. The gauge fixing shows a non-trivial\\nstructure, and some non-intuitive possibilities are listed. Despite the fact\\nthat the theory is not topological, the energy-momentum tensor vanishes\\non-shell, which justifies the quasi-topological appellation we give to this\\ntheory. We show that the theory has three degrees of freedom. Moreover we find\\nan interesting physical interpretation, which consists in a generalized planar\\nelectromagnetism and in the emergence of two vector charges with restricted\\nmobility. These are typical fractonic behaviours which can be related to the so\\ncalled traceless scalar and vector charge theories.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We consider the theory of a generic rank-2 tensor field in three spacetime\\ndimensions, which involves a symmetric tensor field transforming under\\ninfinitesimal diffeomorphisms, and a vector field, whose gauge transformation\\ndepends on a local vector parameter. The gauge fixing shows a non-trivial\\nstructure, and some non-intuitive possibilities are listed. Despite the fact\\nthat the theory is not topological, the energy-momentum tensor vanishes\\non-shell, which justifies the quasi-topological appellation we give to this\\ntheory. We show that the theory has three degrees of freedom. Moreover we find\\nan interesting physical interpretation, which consists in a generalized planar\\nelectromagnetism and in the emergence of two vector charges with restricted\\nmobility. These are typical fractonic behaviours which can be related to the so\\ncalled traceless scalar and vector charge theories.'}, 'authors': [{'name': 'Erica Bertolini'}, {'name': 'Alberto Blasi'}, {'name': 'Nicola Maggiore'}], 'author_detail': {'name': 'Nicola Maggiore'}, 'author': 'Nicola Maggiore', 'arxiv_comment': '30 pages, version to appear on The European Physical Journal C (EPJ\\n  C)', 'links': [{'href': 'http://arxiv.org/abs/2501.01944v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01944v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cond-mat.mes-hall', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'gr-qc', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n",
      "{'entry_id': 'http://arxiv.org/abs/2501.01942v1', 'updated': datetime.datetime(2025, 1, 3, 18, 44, 17, tzinfo=datetime.timezone.utc), 'published': datetime.datetime(2025, 1, 3, 18, 44, 17, tzinfo=datetime.timezone.utc), 'title': 'Hybrid-z: Enhancing Kilo-Degree Survey bright galaxy sample photometric redshifts with deep learning', 'authors': [arxiv.Result.Author('Anjitha John William'), arxiv.Result.Author('Priyanka Jalan'), arxiv.Result.Author('Maciej Bilicki'), arxiv.Result.Author('Wojciech A. Hellwing'), arxiv.Result.Author('Hareesh Thuruthipilly'), arxiv.Result.Author('Szymon J. Nakoneczny')], 'summary': 'We employ deep learning (DL) to improve photometric redshifts (photo-$z$s) in\\nthe Kilo-Degree Survey Data Release 4 Bright galaxy sample (KiDS-Bright DR4).\\nThis dataset, used as a foreground for KiDS lensing and clustering studies, is\\nflux-limited to $r<20$ mag with mean $z=0.23$ and covers 1000 deg$^2$. Its\\nphoto-$z$s were previously derived with artificial neural networks from the\\nANNz2 package, trained on the Galaxy And Mass Assembly (GAMA) spectroscopy.\\nHere we considerably improve over these previous redshift estimations by\\nbuilding a DL model, Hybrid-z, which combines four-band KiDS images with\\nnine-band magnitudes from KiDS+VIKING. The Hybrid-z framework provides\\nphoto-$z$s for KiDS-Bright, with negligible mean residuals of O($10^{-4}$) and\\nscatter at the level of $0.014(1+z)$ -- reduction by 20% over the previous\\nnine-band derivations with ANNz2. We check our photo-$z$ model performance on\\ntest data drawn from GAMA, as well as from other KiDS-overlapping wide-angle\\nspectroscopic surveys, namely SDSS, 2dFLenS, and 2dFGRS. We find stable\\nbehavior and consistent improvement over ANNz2 throughout. We finally apply\\nHybrid-z trained on GAMA to the entire KiDS-Bright DR4 sample of 1.2 million\\ngalaxies. For these final predictions, we design a method of smoothing the\\ninput redshift distribution of the training set, to avoid propagation of\\nfeatures present in GAMA, related to its small sky area and large-scale\\nstructure imprint in its fields. Our work paves the way towards the\\nbest-possible photo-$z$s achievable with machine learning for any galaxy type\\nboth for the final KiDS-Bright DR5 data and for future deeper imaging, such as\\nfrom the Legacy Survey of Space and Time.', 'comment': '14 pages, 7 figures', 'journal_ref': None, 'doi': None, 'primary_category': 'astro-ph.CO', 'categories': ['astro-ph.CO'], 'links': [arxiv.Result.Link('http://arxiv.org/abs/2501.01942v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2501.01942v1', title='pdf', rel='related', content_type=None)], 'pdf_url': 'http://arxiv.org/pdf/2501.01942v1', '_raw': {'id': 'http://arxiv.org/abs/2501.01942v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2501.01942v1', 'updated': '2025-01-03T18:44:17Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=44, tm_sec=17, tm_wday=4, tm_yday=3, tm_isdst=0), 'published': '2025-01-03T18:44:17Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=1, tm_mday=3, tm_hour=18, tm_min=44, tm_sec=17, tm_wday=4, tm_yday=3, tm_isdst=0), 'title': 'Hybrid-z: Enhancing Kilo-Degree Survey bright galaxy sample photometric\\n  redshifts with deep learning', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Hybrid-z: Enhancing Kilo-Degree Survey bright galaxy sample photometric\\n  redshifts with deep learning'}, 'summary': 'We employ deep learning (DL) to improve photometric redshifts (photo-$z$s) in\\nthe Kilo-Degree Survey Data Release 4 Bright galaxy sample (KiDS-Bright DR4).\\nThis dataset, used as a foreground for KiDS lensing and clustering studies, is\\nflux-limited to $r<20$ mag with mean $z=0.23$ and covers 1000 deg$^2$. Its\\nphoto-$z$s were previously derived with artificial neural networks from the\\nANNz2 package, trained on the Galaxy And Mass Assembly (GAMA) spectroscopy.\\nHere we considerably improve over these previous redshift estimations by\\nbuilding a DL model, Hybrid-z, which combines four-band KiDS images with\\nnine-band magnitudes from KiDS+VIKING. The Hybrid-z framework provides\\nphoto-$z$s for KiDS-Bright, with negligible mean residuals of O($10^{-4}$) and\\nscatter at the level of $0.014(1+z)$ -- reduction by 20% over the previous\\nnine-band derivations with ANNz2. We check our photo-$z$ model performance on\\ntest data drawn from GAMA, as well as from other KiDS-overlapping wide-angle\\nspectroscopic surveys, namely SDSS, 2dFLenS, and 2dFGRS. We find stable\\nbehavior and consistent improvement over ANNz2 throughout. We finally apply\\nHybrid-z trained on GAMA to the entire KiDS-Bright DR4 sample of 1.2 million\\ngalaxies. For these final predictions, we design a method of smoothing the\\ninput redshift distribution of the training set, to avoid propagation of\\nfeatures present in GAMA, related to its small sky area and large-scale\\nstructure imprint in its fields. Our work paves the way towards the\\nbest-possible photo-$z$s achievable with machine learning for any galaxy type\\nboth for the final KiDS-Bright DR5 data and for future deeper imaging, such as\\nfrom the Legacy Survey of Space and Time.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We employ deep learning (DL) to improve photometric redshifts (photo-$z$s) in\\nthe Kilo-Degree Survey Data Release 4 Bright galaxy sample (KiDS-Bright DR4).\\nThis dataset, used as a foreground for KiDS lensing and clustering studies, is\\nflux-limited to $r<20$ mag with mean $z=0.23$ and covers 1000 deg$^2$. Its\\nphoto-$z$s were previously derived with artificial neural networks from the\\nANNz2 package, trained on the Galaxy And Mass Assembly (GAMA) spectroscopy.\\nHere we considerably improve over these previous redshift estimations by\\nbuilding a DL model, Hybrid-z, which combines four-band KiDS images with\\nnine-band magnitudes from KiDS+VIKING. The Hybrid-z framework provides\\nphoto-$z$s for KiDS-Bright, with negligible mean residuals of O($10^{-4}$) and\\nscatter at the level of $0.014(1+z)$ -- reduction by 20% over the previous\\nnine-band derivations with ANNz2. We check our photo-$z$ model performance on\\ntest data drawn from GAMA, as well as from other KiDS-overlapping wide-angle\\nspectroscopic surveys, namely SDSS, 2dFLenS, and 2dFGRS. We find stable\\nbehavior and consistent improvement over ANNz2 throughout. We finally apply\\nHybrid-z trained on GAMA to the entire KiDS-Bright DR4 sample of 1.2 million\\ngalaxies. For these final predictions, we design a method of smoothing the\\ninput redshift distribution of the training set, to avoid propagation of\\nfeatures present in GAMA, related to its small sky area and large-scale\\nstructure imprint in its fields. Our work paves the way towards the\\nbest-possible photo-$z$s achievable with machine learning for any galaxy type\\nboth for the final KiDS-Bright DR5 data and for future deeper imaging, such as\\nfrom the Legacy Survey of Space and Time.'}, 'authors': [{'name': 'Anjitha John William'}, {'name': 'Priyanka Jalan'}, {'name': 'Maciej Bilicki'}, {'name': 'Wojciech A. Hellwing'}, {'name': 'Hareesh Thuruthipilly'}, {'name': 'Szymon J. Nakoneczny'}], 'author_detail': {'name': 'Szymon J. Nakoneczny'}, 'author': 'Szymon J. Nakoneczny', 'arxiv_comment': '14 pages, 7 figures', 'links': [{'href': 'http://arxiv.org/abs/2501.01942v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2501.01942v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'astro-ph.CO', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'astro-ph.CO', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}}\n"
     ]
    }
   ],
   "source": [
    "for result in client.results(search):\n",
    "    print(vars(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_search(query: str, max_results: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Performs a search on Arxiv for papers corresponding to the specified query and returns the top results.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        max_results: The number of search results to return.  Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        List Dict[str, Any]: A list of articles and corresponding information including title, summary, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query = query,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    for result in client.results(search):\n",
    "        result_dict = vars(result)\n",
    "        \n",
    "        # Convert all fields to JSON serializable types\n",
    "        for key, value in result_dict.items():\n",
    "            if isinstance(value, datetime):\n",
    "                result_dict[key] = value.isoformat()\n",
    "            elif key == \"authors\":  # Handle Author objects\n",
    "                result_dict[key] = [str(author) for author in value]  # Convert each Author to string\n",
    "            elif key == \"links\":  # Handle Link objects\n",
    "                result_dict[key] = [str(link) for link in value]  # Convert each Link to string\n",
    "            elif not isinstance(value, (str, int, float, list, dict, bool, type(None))):\n",
    "                result_dict[key] = str(value)  # Convert any other non-serializable types to string\n",
    "        \n",
    "        results.append(result_dict)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agents and Group Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, register_function, GroupChat, GroupChatManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "    Create a research paper on novel Named Entity Recognition (NER) methods.  Please discuss how Large Language Models (LLMs) can be leveraged as well as classical Natural Language Processing (NLP) and Machine Learning models.  \n",
      "    Compare and contrast the results of these different methods.  Include a methodology, results, discussion and conclusion section in the research document.  Please ensure the research findings are significant and accurate.  Please \n",
      "    utilize relevant articles from Arxiv to support and substantiate your research findings.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ScientificResearchPlanner\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mScientificResearchPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_aZrhVogKm5aFqt66r540eTSY): arxiv_search *****\u001b[0m\n",
      "Arguments: \n",
      "{\"query\": \"Named Entity Recognition Large Language Models\", \"max_results\": 5}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_HNU7BF3hdz6noBskakHNJb5b): arxiv_search *****\u001b[0m\n",
      "Arguments: \n",
      "{\"query\": \"Named Entity Recognition classical NLP\", \"max_results\": 5}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_ivoqUWpgtSw0o2IHUIKMZmbQ): arxiv_search *****\u001b[0m\n",
      "Arguments: \n",
      "{\"query\": \"Named Entity Recognition Machine Learning\", \"max_results\": 5}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: UserProxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION arxiv_search...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION arxiv_search...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION arxiv_search...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_aZrhVogKm5aFqt66r540eTSY) *****\u001b[0m\n",
      "[{\"entry_id\": \"http://arxiv.org/abs/2501.01957v1\", \"updated\": \"2025-01-03T18:59:52+00:00\", \"published\": \"2025-01-03T18:59:52+00:00\", \"title\": \"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction\", \"authors\": [\"Chaoyou Fu\", \"Haojia Lin\", \"Xiong Wang\", \"Yi-Fan Zhang\", \"Yunhang Shen\", \"Xiaoyu Liu\", \"Yangze Li\", \"Zuwei Long\", \"Heting Gao\", \"Ke Li\", \"Xiawu Zheng\", \"Rongrong Ji\", \"Xing Sun\", \"Caifeng Shan\", \"Ran He\"], \"summary\": \"Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.\", \"comment\": \"https://github.com/VITA-MLLM/VITA\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.CV\", \"categories\": [\"cs.CV\", \"cs.SD\", \"eess.AS\"], \"links\": [\"http://arxiv.org/abs/2501.01957v1\", \"http://arxiv.org/pdf/2501.01957v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01957v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01957v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01957v1\", \"updated\": \"2025-01-03T18:59:52Z\", \"updated_parsed\": [2025, 1, 3, 18, 59, 52, 4, 3, 0], \"published\": \"2025-01-03T18:59:52Z\", \"published_parsed\": [2025, 1, 3, 18, 59, 52, 4, 3, 0], \"title\": \"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction\"}, \"summary\": \"Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.\"}, \"authors\": [{\"name\": \"Chaoyou Fu\"}, {\"name\": \"Haojia Lin\"}, {\"name\": \"Xiong Wang\"}, {\"name\": \"Yi-Fan Zhang\"}, {\"name\": \"Yunhang Shen\"}, {\"name\": \"Xiaoyu Liu\"}, {\"name\": \"Yangze Li\"}, {\"name\": \"Zuwei Long\"}, {\"name\": \"Heting Gao\"}, {\"name\": \"Ke Li\"}, {\"name\": \"Xiawu Zheng\"}, {\"name\": \"Rongrong Ji\"}, {\"name\": \"Xing Sun\"}, {\"name\": \"Caifeng Shan\"}, {\"name\": \"Ran He\"}], \"author_detail\": {\"name\": \"Ran He\"}, \"author\": \"Ran He\", \"arxiv_comment\": \"https://github.com/VITA-MLLM/VITA\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01957v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01957v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.SD\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"eess.AS\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01956v1\", \"updated\": \"2025-01-03T18:59:23+00:00\", \"published\": \"2025-01-03T18:59:23+00:00\", \"title\": \"Metadata Conditioning Accelerates Language Model Pre-training\", \"authors\": [\"Tianyu Gao\", \"Alexander Wettig\", \"Luxi He\", \"Yihe Dong\", \"Sadhika Malladi\", \"Danqi Chen\"], \"summary\": \"The vast diversity of styles, domains, and quality levels present in language\\nmodel pre-training corpora is essential in developing general model\\ncapabilities, but efficiently learning and deploying the correct behaviors\\nexemplified in each of these heterogeneous data sources is challenging. To\\naddress this, we propose a new method, termed Metadata Conditioning then\\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\\ntext during training and later uses a cooldown phase with only the standard\\ntext, thereby enabling the model to function normally even without metadata.\\nMeCo significantly accelerates pre-training across different model scales (600M\\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\\ninstance, a 1.6B language model trained with MeCo matches the downstream task\\nperformance of standard pre-training while using 33% less data. Additionally,\\nMeCo enables us to steer language models by conditioning the inference prompt\\non either real or fabricated metadata that encodes the desired properties of\\nthe output: for example, prepending wikipedia.org to reduce harmful generations\\nor factquizmaster.com (fabricated) to improve common knowledge task\\nperformance. We also demonstrate that MeCo is compatible with different types\\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\\ncomputational overhead, and demonstrates promise in producing more capable and\\nsteerable language models.\", \"comment\": \"Code available at https://github.com/princeton-pli/MeCo\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.CL\", \"categories\": [\"cs.CL\"], \"links\": [\"http://arxiv.org/abs/2501.01956v1\", \"http://arxiv.org/pdf/2501.01956v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01956v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01956v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01956v1\", \"updated\": \"2025-01-03T18:59:23Z\", \"updated_parsed\": [2025, 1, 3, 18, 59, 23, 4, 3, 0], \"published\": \"2025-01-03T18:59:23Z\", \"published_parsed\": [2025, 1, 3, 18, 59, 23, 4, 3, 0], \"title\": \"Metadata Conditioning Accelerates Language Model Pre-training\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Metadata Conditioning Accelerates Language Model Pre-training\"}, \"summary\": \"The vast diversity of styles, domains, and quality levels present in language\\nmodel pre-training corpora is essential in developing general model\\ncapabilities, but efficiently learning and deploying the correct behaviors\\nexemplified in each of these heterogeneous data sources is challenging. To\\naddress this, we propose a new method, termed Metadata Conditioning then\\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\\ntext during training and later uses a cooldown phase with only the standard\\ntext, thereby enabling the model to function normally even without metadata.\\nMeCo significantly accelerates pre-training across different model scales (600M\\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\\ninstance, a 1.6B language model trained with MeCo matches the downstream task\\nperformance of standard pre-training while using 33% less data. Additionally,\\nMeCo enables us to steer language models by conditioning the inference prompt\\non either real or fabricated metadata that encodes the desired properties of\\nthe output: for example, prepending wikipedia.org to reduce harmful generations\\nor factquizmaster.com (fabricated) to improve common knowledge task\\nperformance. We also demonstrate that MeCo is compatible with different types\\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\\ncomputational overhead, and demonstrates promise in producing more capable and\\nsteerable language models.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"The vast diversity of styles, domains, and quality levels present in language\\nmodel pre-training corpora is essential in developing general model\\ncapabilities, but efficiently learning and deploying the correct behaviors\\nexemplified in each of these heterogeneous data sources is challenging. To\\naddress this, we propose a new method, termed Metadata Conditioning then\\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\\ntext during training and later uses a cooldown phase with only the standard\\ntext, thereby enabling the model to function normally even without metadata.\\nMeCo significantly accelerates pre-training across different model scales (600M\\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\\ninstance, a 1.6B language model trained with MeCo matches the downstream task\\nperformance of standard pre-training while using 33% less data. Additionally,\\nMeCo enables us to steer language models by conditioning the inference prompt\\non either real or fabricated metadata that encodes the desired properties of\\nthe output: for example, prepending wikipedia.org to reduce harmful generations\\nor factquizmaster.com (fabricated) to improve common knowledge task\\nperformance. We also demonstrate that MeCo is compatible with different types\\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\\ncomputational overhead, and demonstrates promise in producing more capable and\\nsteerable language models.\"}, \"authors\": [{\"name\": \"Tianyu Gao\"}, {\"name\": \"Alexander Wettig\"}, {\"name\": \"Luxi He\"}, {\"name\": \"Yihe Dong\"}, {\"name\": \"Sadhika Malladi\"}, {\"name\": \"Danqi Chen\"}], \"author_detail\": {\"name\": \"Danqi Chen\"}, \"author\": \"Danqi Chen\", \"arxiv_comment\": \"Code available at https://github.com/princeton-pli/MeCo\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01956v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01956v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.CL\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.CL\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01950v1\", \"updated\": \"2025-01-03T18:54:26+00:00\", \"published\": \"2025-01-03T18:54:26+00:00\", \"title\": \"MADGEN -- Mass-Spec attends to De Novo Molecular generation\", \"authors\": [\"Yinkai Wang\", \"Xiaohui Chen\", \"Liping Liu\", \"Soha Hassoun\"], \"summary\": \"The annotation (assigning structural chemical identities) of MS/MS spectra\\nremains a significant challenge due to the enormous molecular diversity in\\nbiological samples and the limited scope of reference databases. Currently, the\\nvast majority of spectral measurements remain in the \\\"dark chemical space\\\"\\nwithout structural annotations. To improve annotation, we propose MADGEN\\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\\nfor de novo molecular structure generation guided by mass spectrometry data.\\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\\nmolecular generation starting with the scaffold. In the first stage, given an\\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\\nspectrum to guide an attention-based generative model to generate the final\\nmolecule. Our approach constrains the molecular generation search space,\\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\\nperformance with a predictive scaffold retriever and with an oracle retriever.\\nWe demonstrate the effectiveness of using attention to integrate spectral\\ninformation throughout the generation process to achieve strong results with\\nthe oracle retriever.\", \"comment\": \"preprint\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.LG\", \"categories\": [\"cs.LG\", \"cs.AI\"], \"links\": [\"http://arxiv.org/abs/2501.01950v1\", \"http://arxiv.org/pdf/2501.01950v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01950v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01950v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01950v1\", \"updated\": \"2025-01-03T18:54:26Z\", \"updated_parsed\": [2025, 1, 3, 18, 54, 26, 4, 3, 0], \"published\": \"2025-01-03T18:54:26Z\", \"published_parsed\": [2025, 1, 3, 18, 54, 26, 4, 3, 0], \"title\": \"MADGEN -- Mass-Spec attends to De Novo Molecular generation\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"MADGEN -- Mass-Spec attends to De Novo Molecular generation\"}, \"summary\": \"The annotation (assigning structural chemical identities) of MS/MS spectra\\nremains a significant challenge due to the enormous molecular diversity in\\nbiological samples and the limited scope of reference databases. Currently, the\\nvast majority of spectral measurements remain in the \\\"dark chemical space\\\"\\nwithout structural annotations. To improve annotation, we propose MADGEN\\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\\nfor de novo molecular structure generation guided by mass spectrometry data.\\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\\nmolecular generation starting with the scaffold. In the first stage, given an\\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\\nspectrum to guide an attention-based generative model to generate the final\\nmolecule. Our approach constrains the molecular generation search space,\\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\\nperformance with a predictive scaffold retriever and with an oracle retriever.\\nWe demonstrate the effectiveness of using attention to integrate spectral\\ninformation throughout the generation process to achieve strong results with\\nthe oracle retriever.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"The annotation (assigning structural chemical identities) of MS/MS spectra\\nremains a significant challenge due to the enormous molecular diversity in\\nbiological samples and the limited scope of reference databases. Currently, the\\nvast majority of spectral measurements remain in the \\\"dark chemical space\\\"\\nwithout structural annotations. To improve annotation, we propose MADGEN\\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\\nfor de novo molecular structure generation guided by mass spectrometry data.\\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\\nmolecular generation starting with the scaffold. In the first stage, given an\\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\\nspectrum to guide an attention-based generative model to generate the final\\nmolecule. Our approach constrains the molecular generation search space,\\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\\nperformance with a predictive scaffold retriever and with an oracle retriever.\\nWe demonstrate the effectiveness of using attention to integrate spectral\\ninformation throughout the generation process to achieve strong results with\\nthe oracle retriever.\"}, \"authors\": [{\"name\": \"Yinkai Wang\"}, {\"name\": \"Xiaohui Chen\"}, {\"name\": \"Liping Liu\"}, {\"name\": \"Soha Hassoun\"}], \"author_detail\": {\"name\": \"Soha Hassoun\"}, \"author\": \"Soha Hassoun\", \"arxiv_comment\": \"preprint\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01950v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01950v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.AI\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01949v1\", \"updated\": \"2025-01-03T18:52:36+00:00\", \"published\": \"2025-01-03T18:52:36+00:00\", \"title\": \"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment\", \"authors\": [\"Wenyan Cong\", \"Kevin Wang\", \"Jiahui Lei\", \"Colton Stearns\", \"Yuanhao Cai\", \"Dilin Wang\", \"Rakesh Ranjan\", \"Matt Feiszli\", \"Leonidas Guibas\", \"Zhangyang Wang\", \"Weiyao Wang\", \"Zhiwen Fan\"], \"summary\": \"Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.\", \"comment\": \"project page: https://videolifter.github.io\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.CV\", \"categories\": [\"cs.CV\"], \"links\": [\"http://arxiv.org/abs/2501.01949v1\", \"http://arxiv.org/pdf/2501.01949v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01949v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01949v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01949v1\", \"updated\": \"2025-01-03T18:52:36Z\", \"updated_parsed\": [2025, 1, 3, 18, 52, 36, 4, 3, 0], \"published\": \"2025-01-03T18:52:36Z\", \"published_parsed\": [2025, 1, 3, 18, 52, 36, 4, 3, 0], \"title\": \"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\\n  Alignment\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\\n  Alignment\"}, \"summary\": \"Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.\"}, \"authors\": [{\"name\": \"Wenyan Cong\"}, {\"name\": \"Kevin Wang\"}, {\"name\": \"Jiahui Lei\"}, {\"name\": \"Colton Stearns\"}, {\"name\": \"Yuanhao Cai\"}, {\"name\": \"Dilin Wang\"}, {\"name\": \"Rakesh Ranjan\"}, {\"name\": \"Matt Feiszli\"}, {\"name\": \"Leonidas Guibas\"}, {\"name\": \"Zhangyang Wang\"}, {\"name\": \"Weiyao Wang\"}, {\"name\": \"Zhiwen Fan\"}], \"author_detail\": {\"name\": \"Zhiwen Fan\"}, \"author\": \"Zhiwen Fan\", \"arxiv_comment\": \"project page: https://videolifter.github.io\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01949v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01949v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01948v1\", \"updated\": \"2025-01-03T18:52:06+00:00\", \"published\": \"2025-01-03T18:52:06+00:00\", \"title\": \"A New Approach to the Analysis of Cosmological Parameters in Multifield Cosmology\", \"authors\": [\"Katerina Bolshakova\", \"Sergey Chervon\"], \"summary\": \"Currently, a method has been developed for the cosmological inflation model\\nwith a single scalar field to calculate cosmological parameters such as the\\npower spectrum of scalar and tensor perturbations, their spectral indices, and\\nthe tensor-to-scalar ratio. However, for a multifield configuration, a\\ndefinitive method for calculating cosmological parameters has not yet been\\nestablished.\\n  We propose a new effective algorithm for determining cosmological parameters\\nwithin the tensor-multi-scalar theory of gravity, which describes the early\\ninflationary epoch of the Universe. Our approach is based on utilizing a\\nspecific analytical solution within a multifield cosmological model to\\nestablish functional relationships between fields. This method enables the\\ncomputation of cosmological parameters and their comparison with observational\\ndata.\", \"comment\": \"12 pages\", \"journal_ref\": \"Nonlinear World, 2024, Vol. 22, No. 3, pp. 91-103\", \"doi\": null, \"primary_category\": \"gr-qc\", \"categories\": [\"gr-qc\"], \"links\": [\"http://arxiv.org/abs/2501.01948v1\", \"http://arxiv.org/pdf/2501.01948v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01948v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01948v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01948v1\", \"updated\": \"2025-01-03T18:52:06Z\", \"updated_parsed\": [2025, 1, 3, 18, 52, 6, 4, 3, 0], \"published\": \"2025-01-03T18:52:06Z\", \"published_parsed\": [2025, 1, 3, 18, 52, 6, 4, 3, 0], \"title\": \"A New Approach to the Analysis of Cosmological Parameters in Multifield\\n  Cosmology\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"A New Approach to the Analysis of Cosmological Parameters in Multifield\\n  Cosmology\"}, \"summary\": \"Currently, a method has been developed for the cosmological inflation model\\nwith a single scalar field to calculate cosmological parameters such as the\\npower spectrum of scalar and tensor perturbations, their spectral indices, and\\nthe tensor-to-scalar ratio. However, for a multifield configuration, a\\ndefinitive method for calculating cosmological parameters has not yet been\\nestablished.\\n  We propose a new effective algorithm for determining cosmological parameters\\nwithin the tensor-multi-scalar theory of gravity, which describes the early\\ninflationary epoch of the Universe. Our approach is based on utilizing a\\nspecific analytical solution within a multifield cosmological model to\\nestablish functional relationships between fields. This method enables the\\ncomputation of cosmological parameters and their comparison with observational\\ndata.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Currently, a method has been developed for the cosmological inflation model\\nwith a single scalar field to calculate cosmological parameters such as the\\npower spectrum of scalar and tensor perturbations, their spectral indices, and\\nthe tensor-to-scalar ratio. However, for a multifield configuration, a\\ndefinitive method for calculating cosmological parameters has not yet been\\nestablished.\\n  We propose a new effective algorithm for determining cosmological parameters\\nwithin the tensor-multi-scalar theory of gravity, which describes the early\\ninflationary epoch of the Universe. Our approach is based on utilizing a\\nspecific analytical solution within a multifield cosmological model to\\nestablish functional relationships between fields. This method enables the\\ncomputation of cosmological parameters and their comparison with observational\\ndata.\"}, \"authors\": [{\"name\": \"Katerina Bolshakova\"}, {\"name\": \"Sergey Chervon\"}], \"author_detail\": {\"name\": \"Sergey Chervon\"}, \"author\": \"Sergey Chervon\", \"arxiv_comment\": \"12 pages\", \"arxiv_journal_ref\": \"Nonlinear World, 2024, Vol. 22, No. 3, pp. 91-103\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01948v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01948v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"gr-qc\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"gr-qc\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}]\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_HNU7BF3hdz6noBskakHNJb5b) *****\u001b[0m\n",
      "[{\"entry_id\": \"http://arxiv.org/abs/2501.01957v1\", \"updated\": \"2025-01-03T18:59:52+00:00\", \"published\": \"2025-01-03T18:59:52+00:00\", \"title\": \"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction\", \"authors\": [\"Chaoyou Fu\", \"Haojia Lin\", \"Xiong Wang\", \"Yi-Fan Zhang\", \"Yunhang Shen\", \"Xiaoyu Liu\", \"Yangze Li\", \"Zuwei Long\", \"Heting Gao\", \"Ke Li\", \"Xiawu Zheng\", \"Rongrong Ji\", \"Xing Sun\", \"Caifeng Shan\", \"Ran He\"], \"summary\": \"Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.\", \"comment\": \"https://github.com/VITA-MLLM/VITA\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.CV\", \"categories\": [\"cs.CV\", \"cs.SD\", \"eess.AS\"], \"links\": [\"http://arxiv.org/abs/2501.01957v1\", \"http://arxiv.org/pdf/2501.01957v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01957v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01957v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01957v1\", \"updated\": \"2025-01-03T18:59:52Z\", \"updated_parsed\": [2025, 1, 3, 18, 59, 52, 4, 3, 0], \"published\": \"2025-01-03T18:59:52Z\", \"published_parsed\": [2025, 1, 3, 18, 59, 52, 4, 3, 0], \"title\": \"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction\"}, \"summary\": \"Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.\"}, \"authors\": [{\"name\": \"Chaoyou Fu\"}, {\"name\": \"Haojia Lin\"}, {\"name\": \"Xiong Wang\"}, {\"name\": \"Yi-Fan Zhang\"}, {\"name\": \"Yunhang Shen\"}, {\"name\": \"Xiaoyu Liu\"}, {\"name\": \"Yangze Li\"}, {\"name\": \"Zuwei Long\"}, {\"name\": \"Heting Gao\"}, {\"name\": \"Ke Li\"}, {\"name\": \"Xiawu Zheng\"}, {\"name\": \"Rongrong Ji\"}, {\"name\": \"Xing Sun\"}, {\"name\": \"Caifeng Shan\"}, {\"name\": \"Ran He\"}], \"author_detail\": {\"name\": \"Ran He\"}, \"author\": \"Ran He\", \"arxiv_comment\": \"https://github.com/VITA-MLLM/VITA\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01957v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01957v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.SD\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"eess.AS\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01949v1\", \"updated\": \"2025-01-03T18:52:36+00:00\", \"published\": \"2025-01-03T18:52:36+00:00\", \"title\": \"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment\", \"authors\": [\"Wenyan Cong\", \"Kevin Wang\", \"Jiahui Lei\", \"Colton Stearns\", \"Yuanhao Cai\", \"Dilin Wang\", \"Rakesh Ranjan\", \"Matt Feiszli\", \"Leonidas Guibas\", \"Zhangyang Wang\", \"Weiyao Wang\", \"Zhiwen Fan\"], \"summary\": \"Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.\", \"comment\": \"project page: https://videolifter.github.io\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.CV\", \"categories\": [\"cs.CV\"], \"links\": [\"http://arxiv.org/abs/2501.01949v1\", \"http://arxiv.org/pdf/2501.01949v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01949v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01949v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01949v1\", \"updated\": \"2025-01-03T18:52:36Z\", \"updated_parsed\": [2025, 1, 3, 18, 52, 36, 4, 3, 0], \"published\": \"2025-01-03T18:52:36Z\", \"published_parsed\": [2025, 1, 3, 18, 52, 36, 4, 3, 0], \"title\": \"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\\n  Alignment\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\\n  Alignment\"}, \"summary\": \"Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.\"}, \"authors\": [{\"name\": \"Wenyan Cong\"}, {\"name\": \"Kevin Wang\"}, {\"name\": \"Jiahui Lei\"}, {\"name\": \"Colton Stearns\"}, {\"name\": \"Yuanhao Cai\"}, {\"name\": \"Dilin Wang\"}, {\"name\": \"Rakesh Ranjan\"}, {\"name\": \"Matt Feiszli\"}, {\"name\": \"Leonidas Guibas\"}, {\"name\": \"Zhangyang Wang\"}, {\"name\": \"Weiyao Wang\"}, {\"name\": \"Zhiwen Fan\"}], \"author_detail\": {\"name\": \"Zhiwen Fan\"}, \"author\": \"Zhiwen Fan\", \"arxiv_comment\": \"project page: https://videolifter.github.io\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01949v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01949v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01942v1\", \"updated\": \"2025-01-03T18:44:17+00:00\", \"published\": \"2025-01-03T18:44:17+00:00\", \"title\": \"Hybrid-z: Enhancing Kilo-Degree Survey bright galaxy sample photometric redshifts with deep learning\", \"authors\": [\"Anjitha John William\", \"Priyanka Jalan\", \"Maciej Bilicki\", \"Wojciech A. Hellwing\", \"Hareesh Thuruthipilly\", \"Szymon J. Nakoneczny\"], \"summary\": \"We employ deep learning (DL) to improve photometric redshifts (photo-$z$s) in\\nthe Kilo-Degree Survey Data Release 4 Bright galaxy sample (KiDS-Bright DR4).\\nThis dataset, used as a foreground for KiDS lensing and clustering studies, is\\nflux-limited to $r<20$ mag with mean $z=0.23$ and covers 1000 deg$^2$. Its\\nphoto-$z$s were previously derived with artificial neural networks from the\\nANNz2 package, trained on the Galaxy And Mass Assembly (GAMA) spectroscopy.\\nHere we considerably improve over these previous redshift estimations by\\nbuilding a DL model, Hybrid-z, which combines four-band KiDS images with\\nnine-band magnitudes from KiDS+VIKING. The Hybrid-z framework provides\\nphoto-$z$s for KiDS-Bright, with negligible mean residuals of O($10^{-4}$) and\\nscatter at the level of $0.014(1+z)$ -- reduction by 20% over the previous\\nnine-band derivations with ANNz2. We check our photo-$z$ model performance on\\ntest data drawn from GAMA, as well as from other KiDS-overlapping wide-angle\\nspectroscopic surveys, namely SDSS, 2dFLenS, and 2dFGRS. We find stable\\nbehavior and consistent improvement over ANNz2 throughout. We finally apply\\nHybrid-z trained on GAMA to the entire KiDS-Bright DR4 sample of 1.2 million\\ngalaxies. For these final predictions, we design a method of smoothing the\\ninput redshift distribution of the training set, to avoid propagation of\\nfeatures present in GAMA, related to its small sky area and large-scale\\nstructure imprint in its fields. Our work paves the way towards the\\nbest-possible photo-$z$s achievable with machine learning for any galaxy type\\nboth for the final KiDS-Bright DR5 data and for future deeper imaging, such as\\nfrom the Legacy Survey of Space and Time.\", \"comment\": \"14 pages, 7 figures\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"astro-ph.CO\", \"categories\": [\"astro-ph.CO\"], \"links\": [\"http://arxiv.org/abs/2501.01942v1\", \"http://arxiv.org/pdf/2501.01942v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01942v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01942v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01942v1\", \"updated\": \"2025-01-03T18:44:17Z\", \"updated_parsed\": [2025, 1, 3, 18, 44, 17, 4, 3, 0], \"published\": \"2025-01-03T18:44:17Z\", \"published_parsed\": [2025, 1, 3, 18, 44, 17, 4, 3, 0], \"title\": \"Hybrid-z: Enhancing Kilo-Degree Survey bright galaxy sample photometric\\n  redshifts with deep learning\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Hybrid-z: Enhancing Kilo-Degree Survey bright galaxy sample photometric\\n  redshifts with deep learning\"}, \"summary\": \"We employ deep learning (DL) to improve photometric redshifts (photo-$z$s) in\\nthe Kilo-Degree Survey Data Release 4 Bright galaxy sample (KiDS-Bright DR4).\\nThis dataset, used as a foreground for KiDS lensing and clustering studies, is\\nflux-limited to $r<20$ mag with mean $z=0.23$ and covers 1000 deg$^2$. Its\\nphoto-$z$s were previously derived with artificial neural networks from the\\nANNz2 package, trained on the Galaxy And Mass Assembly (GAMA) spectroscopy.\\nHere we considerably improve over these previous redshift estimations by\\nbuilding a DL model, Hybrid-z, which combines four-band KiDS images with\\nnine-band magnitudes from KiDS+VIKING. The Hybrid-z framework provides\\nphoto-$z$s for KiDS-Bright, with negligible mean residuals of O($10^{-4}$) and\\nscatter at the level of $0.014(1+z)$ -- reduction by 20% over the previous\\nnine-band derivations with ANNz2. We check our photo-$z$ model performance on\\ntest data drawn from GAMA, as well as from other KiDS-overlapping wide-angle\\nspectroscopic surveys, namely SDSS, 2dFLenS, and 2dFGRS. We find stable\\nbehavior and consistent improvement over ANNz2 throughout. We finally apply\\nHybrid-z trained on GAMA to the entire KiDS-Bright DR4 sample of 1.2 million\\ngalaxies. For these final predictions, we design a method of smoothing the\\ninput redshift distribution of the training set, to avoid propagation of\\nfeatures present in GAMA, related to its small sky area and large-scale\\nstructure imprint in its fields. Our work paves the way towards the\\nbest-possible photo-$z$s achievable with machine learning for any galaxy type\\nboth for the final KiDS-Bright DR5 data and for future deeper imaging, such as\\nfrom the Legacy Survey of Space and Time.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"We employ deep learning (DL) to improve photometric redshifts (photo-$z$s) in\\nthe Kilo-Degree Survey Data Release 4 Bright galaxy sample (KiDS-Bright DR4).\\nThis dataset, used as a foreground for KiDS lensing and clustering studies, is\\nflux-limited to $r<20$ mag with mean $z=0.23$ and covers 1000 deg$^2$. Its\\nphoto-$z$s were previously derived with artificial neural networks from the\\nANNz2 package, trained on the Galaxy And Mass Assembly (GAMA) spectroscopy.\\nHere we considerably improve over these previous redshift estimations by\\nbuilding a DL model, Hybrid-z, which combines four-band KiDS images with\\nnine-band magnitudes from KiDS+VIKING. The Hybrid-z framework provides\\nphoto-$z$s for KiDS-Bright, with negligible mean residuals of O($10^{-4}$) and\\nscatter at the level of $0.014(1+z)$ -- reduction by 20% over the previous\\nnine-band derivations with ANNz2. We check our photo-$z$ model performance on\\ntest data drawn from GAMA, as well as from other KiDS-overlapping wide-angle\\nspectroscopic surveys, namely SDSS, 2dFLenS, and 2dFGRS. We find stable\\nbehavior and consistent improvement over ANNz2 throughout. We finally apply\\nHybrid-z trained on GAMA to the entire KiDS-Bright DR4 sample of 1.2 million\\ngalaxies. For these final predictions, we design a method of smoothing the\\ninput redshift distribution of the training set, to avoid propagation of\\nfeatures present in GAMA, related to its small sky area and large-scale\\nstructure imprint in its fields. Our work paves the way towards the\\nbest-possible photo-$z$s achievable with machine learning for any galaxy type\\nboth for the final KiDS-Bright DR5 data and for future deeper imaging, such as\\nfrom the Legacy Survey of Space and Time.\"}, \"authors\": [{\"name\": \"Anjitha John William\"}, {\"name\": \"Priyanka Jalan\"}, {\"name\": \"Maciej Bilicki\"}, {\"name\": \"Wojciech A. Hellwing\"}, {\"name\": \"Hareesh Thuruthipilly\"}, {\"name\": \"Szymon J. Nakoneczny\"}], \"author_detail\": {\"name\": \"Szymon J. Nakoneczny\"}, \"author\": \"Szymon J. Nakoneczny\", \"arxiv_comment\": \"14 pages, 7 figures\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01942v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01942v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"astro-ph.CO\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"astro-ph.CO\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01936v1\", \"updated\": \"2025-01-03T18:19:12+00:00\", \"published\": \"2025-01-03T18:19:12+00:00\", \"title\": \"Improving Transducer-Based Spoken Language Understanding with Self-Conditioned CTC and Knowledge Transfer\", \"authors\": [\"Vishal Sunder\", \"Eric Fosler-Lussier\"], \"summary\": \"In this paper, we propose to improve end-to-end (E2E) spoken language\\nunderstand (SLU) in an RNN transducer model (RNN-T) by incorporating a joint\\nself-conditioned CTC automatic speech recognition (ASR) objective. Our proposed\\nmodel is akin to an E2E differentiable cascaded model which performs ASR and\\nSLU sequentially and we ensure that the SLU task is conditioned on the ASR task\\nby having CTC self conditioning. This novel joint modeling of ASR and SLU\\nimproves SLU performance significantly over just using SLU optimization. We\\nfurther improve the performance by aligning the acoustic embeddings of this\\nmodel with the semantically richer BERT model. Our proposed knowledge transfer\\nstrategy makes use of a bag-of-entity prediction layer on the aligned\\nembeddings and the output of this is used to condition the RNN-T based SLU\\ndecoding. These techniques show significant improvement over several strong\\nbaselines and can perform at par with large models like Whisper with\\nsignificantly fewer parameters.\", \"comment\": \"8 pages, 4 figures\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.LG\", \"categories\": [\"cs.LG\"], \"links\": [\"http://arxiv.org/abs/2501.01936v1\", \"http://arxiv.org/pdf/2501.01936v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01936v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01936v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01936v1\", \"updated\": \"2025-01-03T18:19:12Z\", \"updated_parsed\": [2025, 1, 3, 18, 19, 12, 4, 3, 0], \"published\": \"2025-01-03T18:19:12Z\", \"published_parsed\": [2025, 1, 3, 18, 19, 12, 4, 3, 0], \"title\": \"Improving Transducer-Based Spoken Language Understanding with\\n  Self-Conditioned CTC and Knowledge Transfer\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Improving Transducer-Based Spoken Language Understanding with\\n  Self-Conditioned CTC and Knowledge Transfer\"}, \"summary\": \"In this paper, we propose to improve end-to-end (E2E) spoken language\\nunderstand (SLU) in an RNN transducer model (RNN-T) by incorporating a joint\\nself-conditioned CTC automatic speech recognition (ASR) objective. Our proposed\\nmodel is akin to an E2E differentiable cascaded model which performs ASR and\\nSLU sequentially and we ensure that the SLU task is conditioned on the ASR task\\nby having CTC self conditioning. This novel joint modeling of ASR and SLU\\nimproves SLU performance significantly over just using SLU optimization. We\\nfurther improve the performance by aligning the acoustic embeddings of this\\nmodel with the semantically richer BERT model. Our proposed knowledge transfer\\nstrategy makes use of a bag-of-entity prediction layer on the aligned\\nembeddings and the output of this is used to condition the RNN-T based SLU\\ndecoding. These techniques show significant improvement over several strong\\nbaselines and can perform at par with large models like Whisper with\\nsignificantly fewer parameters.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"In this paper, we propose to improve end-to-end (E2E) spoken language\\nunderstand (SLU) in an RNN transducer model (RNN-T) by incorporating a joint\\nself-conditioned CTC automatic speech recognition (ASR) objective. Our proposed\\nmodel is akin to an E2E differentiable cascaded model which performs ASR and\\nSLU sequentially and we ensure that the SLU task is conditioned on the ASR task\\nby having CTC self conditioning. This novel joint modeling of ASR and SLU\\nimproves SLU performance significantly over just using SLU optimization. We\\nfurther improve the performance by aligning the acoustic embeddings of this\\nmodel with the semantically richer BERT model. Our proposed knowledge transfer\\nstrategy makes use of a bag-of-entity prediction layer on the aligned\\nembeddings and the output of this is used to condition the RNN-T based SLU\\ndecoding. These techniques show significant improvement over several strong\\nbaselines and can perform at par with large models like Whisper with\\nsignificantly fewer parameters.\"}, \"authors\": [{\"name\": \"Vishal Sunder\"}, {\"name\": \"Eric Fosler-Lussier\"}], \"author_detail\": {\"name\": \"Eric Fosler-Lussier\"}, \"author\": \"Eric Fosler-Lussier\", \"arxiv_comment\": \"8 pages, 4 figures\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01936v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01936v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01932v1\", \"updated\": \"2025-01-03T18:06:18+00:00\", \"published\": \"2025-01-03T18:06:18+00:00\", \"title\": \"Bridging Classification and Segmentation in Osteosarcoma Assessment via Foundation and Discrete Diffusion Models\", \"authors\": [\"Manh Duong Nguyen\", \"Dac Thai Nguyen\", \"Trung Viet Nguyen\", \"Homi Yamada\", \"Huy Hieu Pham\", \"Phi Le Nguyen\"], \"summary\": \"Osteosarcoma, the most common primary bone cancer, often requires accurate\\nnecrosis assessment from whole slide images (WSIs) for effective treatment\\nplanning and prognosis. However, manual assessments are subjective and prone to\\nvariability. In response, we introduce FDDM, a novel framework bridging the gap\\nbetween patch classification and region-based segmentation. FDDM operates in\\ntwo stages: patch-based classification, followed by region-based refinement,\\nenabling cross-patch information intergation. Leveraging a newly curated\\ndataset of osteosarcoma images, FDDM demonstrates superior segmentation\\nperformance, achieving up to a 10% improvement mIOU and a 32.12% enhancement in\\nnecrosis rate estimation over state-of-the-art methods. This framework sets a\\nnew benchmark in osteosarcoma assessment, highlighting the potential of\\nfoundation models and diffusion-based refinements in complex medical imaging\\ntasks.\", \"comment\": \"Accepted for presentation at the 2025 IEEE International Symposium on\\n  Biomedical Imaging (ISBI 2025)\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.CV\", \"categories\": [\"cs.CV\"], \"links\": [\"http://arxiv.org/abs/2501.01932v1\", \"http://arxiv.org/pdf/2501.01932v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01932v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01932v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01932v1\", \"updated\": \"2025-01-03T18:06:18Z\", \"updated_parsed\": [2025, 1, 3, 18, 6, 18, 4, 3, 0], \"published\": \"2025-01-03T18:06:18Z\", \"published_parsed\": [2025, 1, 3, 18, 6, 18, 4, 3, 0], \"title\": \"Bridging Classification and Segmentation in Osteosarcoma Assessment via\\n  Foundation and Discrete Diffusion Models\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Bridging Classification and Segmentation in Osteosarcoma Assessment via\\n  Foundation and Discrete Diffusion Models\"}, \"summary\": \"Osteosarcoma, the most common primary bone cancer, often requires accurate\\nnecrosis assessment from whole slide images (WSIs) for effective treatment\\nplanning and prognosis. However, manual assessments are subjective and prone to\\nvariability. In response, we introduce FDDM, a novel framework bridging the gap\\nbetween patch classification and region-based segmentation. FDDM operates in\\ntwo stages: patch-based classification, followed by region-based refinement,\\nenabling cross-patch information intergation. Leveraging a newly curated\\ndataset of osteosarcoma images, FDDM demonstrates superior segmentation\\nperformance, achieving up to a 10% improvement mIOU and a 32.12% enhancement in\\nnecrosis rate estimation over state-of-the-art methods. This framework sets a\\nnew benchmark in osteosarcoma assessment, highlighting the potential of\\nfoundation models and diffusion-based refinements in complex medical imaging\\ntasks.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Osteosarcoma, the most common primary bone cancer, often requires accurate\\nnecrosis assessment from whole slide images (WSIs) for effective treatment\\nplanning and prognosis. However, manual assessments are subjective and prone to\\nvariability. In response, we introduce FDDM, a novel framework bridging the gap\\nbetween patch classification and region-based segmentation. FDDM operates in\\ntwo stages: patch-based classification, followed by region-based refinement,\\nenabling cross-patch information intergation. Leveraging a newly curated\\ndataset of osteosarcoma images, FDDM demonstrates superior segmentation\\nperformance, achieving up to a 10% improvement mIOU and a 32.12% enhancement in\\nnecrosis rate estimation over state-of-the-art methods. This framework sets a\\nnew benchmark in osteosarcoma assessment, highlighting the potential of\\nfoundation models and diffusion-based refinements in complex medical imaging\\ntasks.\"}, \"authors\": [{\"name\": \"Manh Duong Nguyen\"}, {\"name\": \"Dac Thai Nguyen\"}, {\"name\": \"Trung Viet Nguyen\"}, {\"name\": \"Homi Yamada\"}, {\"name\": \"Huy Hieu Pham\"}, {\"name\": \"Phi Le Nguyen\"}], \"author_detail\": {\"name\": \"Phi Le Nguyen\"}, \"author\": \"Phi Le Nguyen\", \"arxiv_comment\": \"Accepted for presentation at the 2025 IEEE International Symposium on\\n  Biomedical Imaging (ISBI 2025)\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01932v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01932v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}]\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_ivoqUWpgtSw0o2IHUIKMZmbQ) *****\u001b[0m\n",
      "[{\"entry_id\": \"http://arxiv.org/abs/2501.01957v1\", \"updated\": \"2025-01-03T18:59:52+00:00\", \"published\": \"2025-01-03T18:59:52+00:00\", \"title\": \"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction\", \"authors\": [\"Chaoyou Fu\", \"Haojia Lin\", \"Xiong Wang\", \"Yi-Fan Zhang\", \"Yunhang Shen\", \"Xiaoyu Liu\", \"Yangze Li\", \"Zuwei Long\", \"Heting Gao\", \"Ke Li\", \"Xiawu Zheng\", \"Rongrong Ji\", \"Xing Sun\", \"Caifeng Shan\", \"Ran He\"], \"summary\": \"Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.\", \"comment\": \"https://github.com/VITA-MLLM/VITA\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.CV\", \"categories\": [\"cs.CV\", \"cs.SD\", \"eess.AS\"], \"links\": [\"http://arxiv.org/abs/2501.01957v1\", \"http://arxiv.org/pdf/2501.01957v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01957v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01957v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01957v1\", \"updated\": \"2025-01-03T18:59:52Z\", \"updated_parsed\": [2025, 1, 3, 18, 59, 52, 4, 3, 0], \"published\": \"2025-01-03T18:59:52Z\", \"published_parsed\": [2025, 1, 3, 18, 59, 52, 4, 3, 0], \"title\": \"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction\"}, \"summary\": \"Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.\"}, \"authors\": [{\"name\": \"Chaoyou Fu\"}, {\"name\": \"Haojia Lin\"}, {\"name\": \"Xiong Wang\"}, {\"name\": \"Yi-Fan Zhang\"}, {\"name\": \"Yunhang Shen\"}, {\"name\": \"Xiaoyu Liu\"}, {\"name\": \"Yangze Li\"}, {\"name\": \"Zuwei Long\"}, {\"name\": \"Heting Gao\"}, {\"name\": \"Ke Li\"}, {\"name\": \"Xiawu Zheng\"}, {\"name\": \"Rongrong Ji\"}, {\"name\": \"Xing Sun\"}, {\"name\": \"Caifeng Shan\"}, {\"name\": \"Ran He\"}], \"author_detail\": {\"name\": \"Ran He\"}, \"author\": \"Ran He\", \"arxiv_comment\": \"https://github.com/VITA-MLLM/VITA\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01957v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01957v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.SD\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"eess.AS\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01956v1\", \"updated\": \"2025-01-03T18:59:23+00:00\", \"published\": \"2025-01-03T18:59:23+00:00\", \"title\": \"Metadata Conditioning Accelerates Language Model Pre-training\", \"authors\": [\"Tianyu Gao\", \"Alexander Wettig\", \"Luxi He\", \"Yihe Dong\", \"Sadhika Malladi\", \"Danqi Chen\"], \"summary\": \"The vast diversity of styles, domains, and quality levels present in language\\nmodel pre-training corpora is essential in developing general model\\ncapabilities, but efficiently learning and deploying the correct behaviors\\nexemplified in each of these heterogeneous data sources is challenging. To\\naddress this, we propose a new method, termed Metadata Conditioning then\\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\\ntext during training and later uses a cooldown phase with only the standard\\ntext, thereby enabling the model to function normally even without metadata.\\nMeCo significantly accelerates pre-training across different model scales (600M\\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\\ninstance, a 1.6B language model trained with MeCo matches the downstream task\\nperformance of standard pre-training while using 33% less data. Additionally,\\nMeCo enables us to steer language models by conditioning the inference prompt\\non either real or fabricated metadata that encodes the desired properties of\\nthe output: for example, prepending wikipedia.org to reduce harmful generations\\nor factquizmaster.com (fabricated) to improve common knowledge task\\nperformance. We also demonstrate that MeCo is compatible with different types\\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\\ncomputational overhead, and demonstrates promise in producing more capable and\\nsteerable language models.\", \"comment\": \"Code available at https://github.com/princeton-pli/MeCo\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.CL\", \"categories\": [\"cs.CL\"], \"links\": [\"http://arxiv.org/abs/2501.01956v1\", \"http://arxiv.org/pdf/2501.01956v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01956v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01956v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01956v1\", \"updated\": \"2025-01-03T18:59:23Z\", \"updated_parsed\": [2025, 1, 3, 18, 59, 23, 4, 3, 0], \"published\": \"2025-01-03T18:59:23Z\", \"published_parsed\": [2025, 1, 3, 18, 59, 23, 4, 3, 0], \"title\": \"Metadata Conditioning Accelerates Language Model Pre-training\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Metadata Conditioning Accelerates Language Model Pre-training\"}, \"summary\": \"The vast diversity of styles, domains, and quality levels present in language\\nmodel pre-training corpora is essential in developing general model\\ncapabilities, but efficiently learning and deploying the correct behaviors\\nexemplified in each of these heterogeneous data sources is challenging. To\\naddress this, we propose a new method, termed Metadata Conditioning then\\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\\ntext during training and later uses a cooldown phase with only the standard\\ntext, thereby enabling the model to function normally even without metadata.\\nMeCo significantly accelerates pre-training across different model scales (600M\\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\\ninstance, a 1.6B language model trained with MeCo matches the downstream task\\nperformance of standard pre-training while using 33% less data. Additionally,\\nMeCo enables us to steer language models by conditioning the inference prompt\\non either real or fabricated metadata that encodes the desired properties of\\nthe output: for example, prepending wikipedia.org to reduce harmful generations\\nor factquizmaster.com (fabricated) to improve common knowledge task\\nperformance. We also demonstrate that MeCo is compatible with different types\\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\\ncomputational overhead, and demonstrates promise in producing more capable and\\nsteerable language models.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"The vast diversity of styles, domains, and quality levels present in language\\nmodel pre-training corpora is essential in developing general model\\ncapabilities, but efficiently learning and deploying the correct behaviors\\nexemplified in each of these heterogeneous data sources is challenging. To\\naddress this, we propose a new method, termed Metadata Conditioning then\\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\\ntext during training and later uses a cooldown phase with only the standard\\ntext, thereby enabling the model to function normally even without metadata.\\nMeCo significantly accelerates pre-training across different model scales (600M\\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\\ninstance, a 1.6B language model trained with MeCo matches the downstream task\\nperformance of standard pre-training while using 33% less data. Additionally,\\nMeCo enables us to steer language models by conditioning the inference prompt\\non either real or fabricated metadata that encodes the desired properties of\\nthe output: for example, prepending wikipedia.org to reduce harmful generations\\nor factquizmaster.com (fabricated) to improve common knowledge task\\nperformance. We also demonstrate that MeCo is compatible with different types\\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\\ncomputational overhead, and demonstrates promise in producing more capable and\\nsteerable language models.\"}, \"authors\": [{\"name\": \"Tianyu Gao\"}, {\"name\": \"Alexander Wettig\"}, {\"name\": \"Luxi He\"}, {\"name\": \"Yihe Dong\"}, {\"name\": \"Sadhika Malladi\"}, {\"name\": \"Danqi Chen\"}], \"author_detail\": {\"name\": \"Danqi Chen\"}, \"author\": \"Danqi Chen\", \"arxiv_comment\": \"Code available at https://github.com/princeton-pli/MeCo\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01956v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01956v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.CL\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.CL\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01951v2\", \"updated\": \"2025-01-06T06:46:07+00:00\", \"published\": \"2025-01-03T18:54:46+00:00\", \"title\": \"MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of Accelerators\", \"authors\": [\"Cheng Wan\", \"Runkai Tao\", \"Zheng Du\", \"Yang Katie Zhao\", \"Yingyan Celine Lin\"], \"summary\": \"Graph convolutional networks (GCNs) have demonstrated superiority in\\ngraph-based learning tasks. However, training GCNs on full graphs is\\nparticularly challenging, due to the following two challenges: (1) the\\nassociated feature tensors can easily explode the memory and block the\\ncommunication bandwidth of modern accelerators, and (2) the computation\\nworkflow in training GCNs alternates between sparse and dense matrix\\noperations, complicating the efficient utilization of computational resources.\\nExisting solutions for scalable distributed full-graph GCN training mostly\\nadopt partition parallelism, which is unsatisfactory as they only partially\\naddress the first challenge while incurring scaled-out communication volume. To\\nthis end, we propose MixGCN aiming to simultaneously address both the\\naforementioned challenges towards GCN training. To tackle the first challenge,\\nMixGCN integrates mixture of parallelism. Both theoretical and empirical\\nanalysis verify its constant communication volumes and enhanced balanced\\nworkload; For handling the second challenge, we consider mixture of\\naccelerators (i.e., sparse and dense accelerators) with a dedicated accelerator\\nfor GCN training and a fine-grain pipeline. Extensive experiments show that\\nMixGCN achieves boosted training efficiency and scalability.\", \"comment\": \"15 pages, 12 figures, 5 tables\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.LG\", \"categories\": [\"cs.LG\", \"cs.AI\"], \"links\": [\"http://arxiv.org/abs/2501.01951v2\", \"http://arxiv.org/pdf/2501.01951v2\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01951v2\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01951v2\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01951v2\", \"updated\": \"2025-01-06T06:46:07Z\", \"updated_parsed\": [2025, 1, 6, 6, 46, 7, 0, 6, 0], \"published\": \"2025-01-03T18:54:46Z\", \"published_parsed\": [2025, 1, 3, 18, 54, 46, 4, 3, 0], \"title\": \"MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of\\n  Accelerators\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of\\n  Accelerators\"}, \"summary\": \"Graph convolutional networks (GCNs) have demonstrated superiority in\\ngraph-based learning tasks. However, training GCNs on full graphs is\\nparticularly challenging, due to the following two challenges: (1) the\\nassociated feature tensors can easily explode the memory and block the\\ncommunication bandwidth of modern accelerators, and (2) the computation\\nworkflow in training GCNs alternates between sparse and dense matrix\\noperations, complicating the efficient utilization of computational resources.\\nExisting solutions for scalable distributed full-graph GCN training mostly\\nadopt partition parallelism, which is unsatisfactory as they only partially\\naddress the first challenge while incurring scaled-out communication volume. To\\nthis end, we propose MixGCN aiming to simultaneously address both the\\naforementioned challenges towards GCN training. To tackle the first challenge,\\nMixGCN integrates mixture of parallelism. Both theoretical and empirical\\nanalysis verify its constant communication volumes and enhanced balanced\\nworkload; For handling the second challenge, we consider mixture of\\naccelerators (i.e., sparse and dense accelerators) with a dedicated accelerator\\nfor GCN training and a fine-grain pipeline. Extensive experiments show that\\nMixGCN achieves boosted training efficiency and scalability.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Graph convolutional networks (GCNs) have demonstrated superiority in\\ngraph-based learning tasks. However, training GCNs on full graphs is\\nparticularly challenging, due to the following two challenges: (1) the\\nassociated feature tensors can easily explode the memory and block the\\ncommunication bandwidth of modern accelerators, and (2) the computation\\nworkflow in training GCNs alternates between sparse and dense matrix\\noperations, complicating the efficient utilization of computational resources.\\nExisting solutions for scalable distributed full-graph GCN training mostly\\nadopt partition parallelism, which is unsatisfactory as they only partially\\naddress the first challenge while incurring scaled-out communication volume. To\\nthis end, we propose MixGCN aiming to simultaneously address both the\\naforementioned challenges towards GCN training. To tackle the first challenge,\\nMixGCN integrates mixture of parallelism. Both theoretical and empirical\\nanalysis verify its constant communication volumes and enhanced balanced\\nworkload; For handling the second challenge, we consider mixture of\\naccelerators (i.e., sparse and dense accelerators) with a dedicated accelerator\\nfor GCN training and a fine-grain pipeline. Extensive experiments show that\\nMixGCN achieves boosted training efficiency and scalability.\"}, \"authors\": [{\"name\": \"Cheng Wan\"}, {\"name\": \"Runkai Tao\"}, {\"name\": \"Zheng Du\"}, {\"name\": \"Yang Katie Zhao\"}, {\"name\": \"Yingyan Celine Lin\"}], \"author_detail\": {\"name\": \"Yingyan Celine Lin\"}, \"author\": \"Yingyan Celine Lin\", \"arxiv_comment\": \"15 pages, 12 figures, 5 tables\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01951v2\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01951v2\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.AI\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01950v1\", \"updated\": \"2025-01-03T18:54:26+00:00\", \"published\": \"2025-01-03T18:54:26+00:00\", \"title\": \"MADGEN -- Mass-Spec attends to De Novo Molecular generation\", \"authors\": [\"Yinkai Wang\", \"Xiaohui Chen\", \"Liping Liu\", \"Soha Hassoun\"], \"summary\": \"The annotation (assigning structural chemical identities) of MS/MS spectra\\nremains a significant challenge due to the enormous molecular diversity in\\nbiological samples and the limited scope of reference databases. Currently, the\\nvast majority of spectral measurements remain in the \\\"dark chemical space\\\"\\nwithout structural annotations. To improve annotation, we propose MADGEN\\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\\nfor de novo molecular structure generation guided by mass spectrometry data.\\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\\nmolecular generation starting with the scaffold. In the first stage, given an\\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\\nspectrum to guide an attention-based generative model to generate the final\\nmolecule. Our approach constrains the molecular generation search space,\\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\\nperformance with a predictive scaffold retriever and with an oracle retriever.\\nWe demonstrate the effectiveness of using attention to integrate spectral\\ninformation throughout the generation process to achieve strong results with\\nthe oracle retriever.\", \"comment\": \"preprint\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.LG\", \"categories\": [\"cs.LG\", \"cs.AI\"], \"links\": [\"http://arxiv.org/abs/2501.01950v1\", \"http://arxiv.org/pdf/2501.01950v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01950v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01950v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01950v1\", \"updated\": \"2025-01-03T18:54:26Z\", \"updated_parsed\": [2025, 1, 3, 18, 54, 26, 4, 3, 0], \"published\": \"2025-01-03T18:54:26Z\", \"published_parsed\": [2025, 1, 3, 18, 54, 26, 4, 3, 0], \"title\": \"MADGEN -- Mass-Spec attends to De Novo Molecular generation\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"MADGEN -- Mass-Spec attends to De Novo Molecular generation\"}, \"summary\": \"The annotation (assigning structural chemical identities) of MS/MS spectra\\nremains a significant challenge due to the enormous molecular diversity in\\nbiological samples and the limited scope of reference databases. Currently, the\\nvast majority of spectral measurements remain in the \\\"dark chemical space\\\"\\nwithout structural annotations. To improve annotation, we propose MADGEN\\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\\nfor de novo molecular structure generation guided by mass spectrometry data.\\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\\nmolecular generation starting with the scaffold. In the first stage, given an\\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\\nspectrum to guide an attention-based generative model to generate the final\\nmolecule. Our approach constrains the molecular generation search space,\\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\\nperformance with a predictive scaffold retriever and with an oracle retriever.\\nWe demonstrate the effectiveness of using attention to integrate spectral\\ninformation throughout the generation process to achieve strong results with\\nthe oracle retriever.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"The annotation (assigning structural chemical identities) of MS/MS spectra\\nremains a significant challenge due to the enormous molecular diversity in\\nbiological samples and the limited scope of reference databases. Currently, the\\nvast majority of spectral measurements remain in the \\\"dark chemical space\\\"\\nwithout structural annotations. To improve annotation, we propose MADGEN\\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\\nfor de novo molecular structure generation guided by mass spectrometry data.\\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\\nmolecular generation starting with the scaffold. In the first stage, given an\\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\\nspectrum to guide an attention-based generative model to generate the final\\nmolecule. Our approach constrains the molecular generation search space,\\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\\nperformance with a predictive scaffold retriever and with an oracle retriever.\\nWe demonstrate the effectiveness of using attention to integrate spectral\\ninformation throughout the generation process to achieve strong results with\\nthe oracle retriever.\"}, \"authors\": [{\"name\": \"Yinkai Wang\"}, {\"name\": \"Xiaohui Chen\"}, {\"name\": \"Liping Liu\"}, {\"name\": \"Soha Hassoun\"}], \"author_detail\": {\"name\": \"Soha Hassoun\"}, \"author\": \"Soha Hassoun\", \"arxiv_comment\": \"preprint\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01950v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01950v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.LG\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}, {\"term\": \"cs.AI\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}, {\"entry_id\": \"http://arxiv.org/abs/2501.01949v1\", \"updated\": \"2025-01-03T18:52:36+00:00\", \"published\": \"2025-01-03T18:52:36+00:00\", \"title\": \"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment\", \"authors\": [\"Wenyan Cong\", \"Kevin Wang\", \"Jiahui Lei\", \"Colton Stearns\", \"Yuanhao Cai\", \"Dilin Wang\", \"Rakesh Ranjan\", \"Matt Feiszli\", \"Leonidas Guibas\", \"Zhangyang Wang\", \"Weiyao Wang\", \"Zhiwen Fan\"], \"summary\": \"Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.\", \"comment\": \"project page: https://videolifter.github.io\", \"journal_ref\": null, \"doi\": null, \"primary_category\": \"cs.CV\", \"categories\": [\"cs.CV\"], \"links\": [\"http://arxiv.org/abs/2501.01949v1\", \"http://arxiv.org/pdf/2501.01949v1\"], \"pdf_url\": \"http://arxiv.org/pdf/2501.01949v1\", \"_raw\": {\"id\": \"http://arxiv.org/abs/2501.01949v1\", \"guidislink\": true, \"link\": \"http://arxiv.org/abs/2501.01949v1\", \"updated\": \"2025-01-03T18:52:36Z\", \"updated_parsed\": [2025, 1, 3, 18, 52, 36, 4, 3, 0], \"published\": \"2025-01-03T18:52:36Z\", \"published_parsed\": [2025, 1, 3, 18, 52, 36, 4, 3, 0], \"title\": \"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\\n  Alignment\", \"title_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\\n  Alignment\"}, \"summary\": \"Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.\", \"summary_detail\": {\"type\": \"text/plain\", \"language\": null, \"base\": \"\", \"value\": \"Efficiently reconstructing accurate 3D models from monocular video is a key\\nchallenge in computer vision, critical for advancing applications in virtual\\nreality, robotics, and scene understanding. Existing approaches typically\\nrequire pre-computed camera parameters and frame-by-frame reconstruction\\npipelines, which are prone to error accumulation and entail significant\\ncomputational overhead. To address these limitations, we introduce VideoLifter,\\na novel framework that leverages geometric priors from a learnable model to\\nincrementally optimize a globally sparse to dense 3D representation directly\\nfrom video sequences. VideoLifter segments the video sequence into local\\nwindows, where it matches and registers frames, constructs consistent\\nfragments, and aligns them hierarchically to produce a unified 3D model. By\\ntracking and propagating sparse point correspondences across frames and\\nfragments, VideoLifter incrementally refines camera poses and 3D structure,\\nminimizing reprojection error for improved accuracy and robustness. This\\napproach significantly accelerates the reconstruction process, reducing\\ntraining time by over 82% while surpassing current state-of-the-art methods in\\nvisual fidelity and computational efficiency.\"}, \"authors\": [{\"name\": \"Wenyan Cong\"}, {\"name\": \"Kevin Wang\"}, {\"name\": \"Jiahui Lei\"}, {\"name\": \"Colton Stearns\"}, {\"name\": \"Yuanhao Cai\"}, {\"name\": \"Dilin Wang\"}, {\"name\": \"Rakesh Ranjan\"}, {\"name\": \"Matt Feiszli\"}, {\"name\": \"Leonidas Guibas\"}, {\"name\": \"Zhangyang Wang\"}, {\"name\": \"Weiyao Wang\"}, {\"name\": \"Zhiwen Fan\"}], \"author_detail\": {\"name\": \"Zhiwen Fan\"}, \"author\": \"Zhiwen Fan\", \"arxiv_comment\": \"project page: https://videolifter.github.io\", \"links\": [{\"href\": \"http://arxiv.org/abs/2501.01949v1\", \"rel\": \"alternate\", \"type\": \"text/html\"}, {\"title\": \"pdf\", \"href\": \"http://arxiv.org/pdf/2501.01949v1\", \"rel\": \"related\", \"type\": \"application/pdf\"}], \"arxiv_primary_category\": {\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\"}, \"tags\": [{\"term\": \"cs.CV\", \"scheme\": \"http://arxiv.org/schemas/atom\", \"label\": null}]}}]\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mFinalReviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUserProxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Update the chat initiation\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m scientific_groupchat_result \u001b[38;5;241m=\u001b[39m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscientific_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_termination_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mterminate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced to prevent excessive conversation\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1107\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1107\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:748\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    746\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 748\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    752\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:914\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 914\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2068\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2068\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2070\u001b[0m         log_event(\n\u001b[1;32m   2071\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2072\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2076\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2077\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:1171\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;66;03m# select the next speaker\u001b[39;00m\n\u001b[0;32m-> 1171\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m \u001b[43mgroupchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m   1173\u001b[0m         iostream \u001b[38;5;241m=\u001b[39m IOStream\u001b[38;5;241m.\u001b[39mget_default()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:565\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_agent(last_speaker)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# auto speaker selection with 2-agent chat\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_select_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_speaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:743\u001b[0m, in \u001b[0;36mGroupChat._auto_select_speaker\u001b[0;34m(self, last_speaker, selector, messages, agents)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_speaker_selection_transforms\u001b[38;5;241m.\u001b[39madd_to_agent(speaker_selection_agent)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Run the speaker selection chat\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchecking_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaker_selection_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't use caching for the speaker selection chat\u001b[39;49;00m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_attempts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Limiting the chat to the number of attempts, including the initial one\u001b[39;49;00m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker_auto_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Base silence on the verbose attribute\u001b[39;49;00m\n\u001b[1;32m    751\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_speaker_selection_result(result, last_speaker, agents)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1107\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1107\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:748\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    746\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 748\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    752\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:914\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 914\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2068\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2068\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2070\u001b[0m         log_event(\n\u001b[1;32m   2071\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2072\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2076\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2077\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1436\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1435\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1436\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1455\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1455\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/oai/client.py:777\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_throttle_api_calls(i)\n\u001b[1;32m    776\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[0;32m--> 777\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    779\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/autogen/oai/client.py:342\u001b[0m, in \u001b[0;36mOpenAIClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    340\u001b[0m     params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    341\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/autogen_playground/lib/python3.12/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Update the FinalReviewer class to properly format the final paper\n",
    "# Extense classes for the medical evaluation process\n",
    "class FinalReviewer(ConversableAgent):\n",
    "    def receive(self, message, sender, request_reply=True, silent=False):\n",
    "        super().receive(message, sender, request_reply, silent)\n",
    "        # Check if the final document is satisfactory\n",
    "        if \"satisfactory\" in message.get(\"content\", \"\").lower():\n",
    "            self.send({\"content\": \"TERMINATE\"}, sender)\n",
    "\n",
    "scientific_research_planner = ConversableAgent(\n",
    "    name = \"ScientificResearchPlanner\",\n",
    "    system_message = \"\"\"Given a research task, your job is to determine the information required to support the research.  Please make a determination whether Arxiv articles are necessary for substantiating research findings.\n",
    "    Please provide clear instructions to the scientific research team.  You have the ability to retrieve relevant articles from Arxiv using the provided tool function.  Please limit the search to a maximum of five topics, as no more than five\n",
    "    search queries to Arxiv are permitted at a time.  Please monitor the task's progress continously and delegate subtasks to other agents as necessary.  If relevant Arxiv articles are not available or do not provide sufficient information to corroborate research\n",
    "    findings, please suggest alternative strategies or sources.  After each step, please check the current progress and instruct the following steps.  If a step fails for some reason, please try a workaround.\n",
    "    \"\"\",\n",
    "    description = \"You are a planner.  Given a scientific research task, determine what information is required to complete the task.  After each step, please check the progress and instruct the next steps.\",\n",
    "    llm_config = llm_config\n",
    ")\n",
    "\n",
    "scientific_reviewer = ConversableAgent(\n",
    "    name = \"ScientificReviewer\",\n",
    "    system_message = \"\"\"You are a scientific reviewer with expertise in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.\n",
    "    Please review the research findings from a validity, accuracy and significance of research findings standpoint.  Please provide concise, relevant and specific suggestions for improvement and consideration.\n",
    "    \"\"\",\n",
    "    llm_config = llm_config\n",
    "\n",
    ")\n",
    "\n",
    "scientific_researcher = ConversableAgent(\n",
    "    name = \"ScientificResearcher\",\n",
    "    system_message = \"\"\"You are a scientific researcher responsible for drafting a scientific manuscript that will detail your research findings.  Please ensure that the content of the scientific manuscript is valid and encapsulates the significant findings from your research.\n",
    "    Please utilize the arxiv_search function for literature, and ensure that you cite at least one relevant source to substantiate your research findings.\n",
    "    \"\"\",\n",
    "    llm_config = llm_config\n",
    ")\n",
    "\n",
    "scientific_editor = ConversableAgent(\n",
    "    name = \"ScientificEditor\",\n",
    "    system_message = \"\"\"You are a scientific editor.  You are responsible for editing the content to ensure that the grammar and diction are correct.  You should also ensure that the scientific terminology referenced in the paper is relevant and accurate.  Please ensure that the paper is coherent.\"\"\",\n",
    "    llm_config = llm_config\n",
    ")\n",
    "\n",
    "final_reviewer = FinalReviewer(\n",
    "    name = \"FinalReviewer\",\n",
    "    system_message = \"\"\"You are the final reviewer, responsible for consolidating and reviewing feedback from the scientific reviewer.  You are responsible for making the ultimate decision on the content's readiness for publication.\n",
    "    Please write TERMINATE if the document meets the required criteria and is ready to be published.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# user proxy for executing tool calls\n",
    "user_proxy = ConversableAgent(\n",
    "    name = \"UserProxy\",\n",
    "    llm_config = False,\n",
    "    is_termination_msg=lambda msg: \"TERMINATE\" in msg.get(\"content\", \"\"),\n",
    "    human_input_mode = \"NEVER\"\n",
    ")\n",
    "\n",
    "# register the arxiv_search tool for the appropriate agents\n",
    "for caller in [scientific_researcher, scientific_research_planner]:\n",
    "    register_function(\n",
    "        arxiv_search,\n",
    "        caller = caller,\n",
    "        executor = user_proxy,\n",
    "        name = \"arxiv_search\",\n",
    "        description = \"Retrives relevant articles from the Arxiv archive based on a search query.\"\n",
    "    )\n",
    "\n",
    "\n",
    "scientific_groupchat = GroupChat(\n",
    "    agents = [\n",
    "        scientific_reviewer, scientific_editor, scientific_research_planner, scientific_researcher, final_reviewer, user_proxy\n",
    "    ],\n",
    "    messages = [],\n",
    "    max_round = 3,\n",
    "    allowed_or_disallowed_speaker_transitions = {\n",
    "        scientific_reviewer: [final_reviewer, scientific_researcher],\n",
    "        scientific_researcher: [scientific_editor, scientific_reviewer],\n",
    "        final_reviewer: [scientific_researcher],  # Final reviewer can terminate or request changes\n",
    "        scientific_editor: [scientific_reviewer, scientific_researcher], \n",
    "        scientific_research_planner: [\n",
    "            scientific_editor, scientific_researcher, scientific_reviewer, final_reviewer,\n",
    "        ],\n",
    "        user_proxy: [\n",
    "            scientific_reviewer, scientific_editor, scientific_research_planner, scientific_researcher, final_reviewer\n",
    "        ]\n",
    "    },\n",
    "    speaker_transitions_type = \"allowed\"\n",
    ")\n",
    "\n",
    "\n",
    "scientific_manager = GroupChatManager(\n",
    "    groupchat = scientific_groupchat, llm_config = llm_config\n",
    ")\n",
    "\n",
    "task = \"\"\"\n",
    "    Create a research paper on novel Named Entity Recognition (NER) methods.  Please discuss how Large Language Models (LLMs) can be leveraged as well as classical Natural Language Processing (NLP) and Machine Learning models.  \n",
    "    Compare and contrast the results of these different methods.  Include a methodology, results, discussion and conclusion section in the research document.  Please ensure the research findings are significant and accurate.  Please \n",
    "    utilize relevant articles from Arxiv to support and substantiate your research findings.\n",
    "\"\"\"\n",
    "\n",
    "# Update the main chat initiation with stricter termination checking\n",
    "def is_termination_msg(x):\n",
    "    content = x.get(\"content\", \"\")\n",
    "    if \"TERMINATE\" in content:\n",
    "        # Extract and print the final paper\n",
    "        if \"FINAL PAPER:\" in content:\n",
    "            paper_start = content.find(\"FINAL PAPER:\")\n",
    "            paper_end = content.find(\"TERMINATE\")\n",
    "            final_paper = content[paper_start:paper_end].strip()\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(final_paper)\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Update the chat initiation\n",
    "scientific_groupchat_result = user_proxy.initiate_chat(\n",
    "    recipient=scientific_manager,\n",
    "    message=task,\n",
    "    is_termination_msg=lambda x: \"terminate\" in x.get(\"content\", \"\").lower(),\n",
    "    max_turns=8  # Reduced to prevent excessive conversation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
